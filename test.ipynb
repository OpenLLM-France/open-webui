{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Linagora\n",
    "    * user1 => sk-gmw4uQR9udvH5lIBXzkwIA (MAIF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stripe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "{'key': 'sk-VTukT47OBfIoc7xuogx-hg', 'info': {'key_name': 'sk-...x-hg', 'key_alias': 'test-key', 'soft_budget_cooldown': False, 'spend': 5.845, 'expires': None, 'models': ['all-team-models'], 'aliases': {}, 'config': {}, 'user_id': 'user-1234', 'team_id': None, 'permissions': {}, 'max_parallel_requests': None, 'metadata': {}, 'blocked': None, 'tpm_limit': None, 'rpm_limit': None, 'max_budget': 20.23, 'budget_duration': None, 'budget_reset_at': None, 'allowed_cache_controls': [], 'model_spend': {}, 'model_max_budget': {}, 'budget_id': None, 'created_at': '2024-12-11T08:57:17.779000+00:00', 'updated_at': '2024-12-11T15:09:58.869000+00:00', 'litellm_budget_table': None}}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = \"http://localhost:4040/key/info\"\n",
    "key = \"sk-VTukT47OBfIoc7xuogx-hg\"\n",
    "auth_token = \"sk-1234\"\n",
    "\n",
    "# Constructing the headers\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {auth_token}\"\n",
    "}\n",
    "\n",
    "# Sending the GET request\n",
    "response = requests.get(url, params={\"key\": key}, headers=headers)\n",
    "\n",
    "# Printing the response\n",
    "print(response.status_code)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'key': 'sk-VTukT47OBfIoc7xuogx-hg',\n",
       " 'info': {'key_name': 'sk-...x-hg',\n",
       "  'key_alias': 'test-key',\n",
       "  'soft_budget_cooldown': False,\n",
       "  'spend': 5.845,\n",
       "  'expires': None,\n",
       "  'models': ['all-team-models'],\n",
       "  'aliases': {},\n",
       "  'config': {},\n",
       "  'user_id': 'user-1234',\n",
       "  'team_id': None,\n",
       "  'permissions': {},\n",
       "  'max_parallel_requests': None,\n",
       "  'metadata': {},\n",
       "  'blocked': None,\n",
       "  'tpm_limit': None,\n",
       "  'rpm_limit': None,\n",
       "  'max_budget': 20.23,\n",
       "  'budget_duration': None,\n",
       "  'budget_reset_at': None,\n",
       "  'allowed_cache_controls': [],\n",
       "  'model_spend': {},\n",
       "  'model_max_budget': {},\n",
       "  'budget_id': None,\n",
       "  'created_at': '2024-12-11T08:57:17.779000+00:00',\n",
       "  'updated_at': '2024-12-11T15:09:58.869000+00:00',\n",
       "  'litellm_budget_table': None}}"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "{'key': 'sk-VTukT47OBfIoc7xuogx-hg', 'token': 'c9f39d62231e157a4dc466d0fee0695860ab364cd52e979e7707306db1b0fb85', 'key_name': 'sk-...x-hg', 'key_alias': 'test-key', 'soft_budget_cooldown': False, 'spend': 5.845, 'expires': None, 'models': ['all-team-models'], 'aliases': {}, 'config': {}, 'user_id': 'user-1234', 'team_id': None, 'permissions': {}, 'max_parallel_requests': None, 'metadata': {}, 'blocked': None, 'tpm_limit': None, 'rpm_limit': None, 'max_budget': 10.23, 'budget_duration': None, 'budget_reset_at': None, 'allowed_cache_controls': [], 'model_spend': {}, 'model_max_budget': {}, 'budget_id': None, 'created_at': '2024-12-11T08:57:17.779000+00:00', 'updated_at': '2024-12-11T14:43:14.861000+00:00', 'litellm_budget_table': None}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "url = \"http://localhost:4040/key/update\"\n",
    "auth_token = \"sk-1234\"\n",
    "\n",
    "# Constructing headers\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {auth_token}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "# Data to be sent in the POST request\n",
    "data = {\n",
    "    \"key\": \"sk-VTukT47OBfIoc7xuogx-hg\",\n",
    "    \"user_id\": \"user-15\",\n",
    "    \"max_budget\": 30.23\n",
    "}\n",
    "\n",
    "# Sending the POST request\n",
    "response = requests.post(url, headers=headers, data=json.dumps(data))\n",
    "\n",
    "# Printing the response\n",
    "print(response.status_code)\n",
    "print(response.json())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400\n",
      "{'detail': 'Invalid signature'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = 'http://localhost:8000/stripe/webhook'\n",
    "\n",
    "\n",
    "headers = {\n",
    "    'Content-Type': 'application/json'\n",
    "}\n",
    "\n",
    "response = requests.post(url, json=payload, headers=headers)\n",
    "print(response.status_code)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'{\"message\":\"Hello\"}'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'http://localhost:8000/stripe/test'\n",
    "response = requests.get(url)\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!doctype html>\n",
      "<html lang=\"en\">\n",
      "\t<head>\n",
      "\t\t<meta charset=\"utf-8\" />\n",
      "\t\t<link rel=\"icon\" type=\"image/png\" href=\"/favicon/favicon-96x96.png\" sizes=\"96x96\" />\n",
      "\t\t<link rel=\"icon\" type=\"image/svg+xml\" href=\"/favicon/favicon.svg\" />\n",
      "\t\t<link rel=\"shortcut icon\" href=\"/favicon/favicon.ico\" />\n",
      "\t\t<link rel=\"apple-touch-icon\" sizes=\"180x180\" href=\"/favicon/apple-touch-icon.png\" />\n",
      "\t\t<meta name=\"apple-mobile-web-app-title\" content=\"Open WebUI\" />\n",
      "\t\t<link rel=\"manifest\" href=\"/favicon/site.webmanifest\" />\n",
      "\t\t<meta\n",
      "\t\t\tname=\"viewport\"\n",
      "\t\t\tcontent=\"width=device-width, initial-scale=1, maximum-scale=1, viewport-fit=cover\"\n",
      "\t\t/>\n",
      "\t\t<meta name=\"theme-color\" content=\"#171717\" />\n",
      "\t\t<meta name=\"robots\" content=\"noindex,nofollow\" />\n",
      "\t\t<meta name=\"description\" content=\"Open WebUI\" />\n",
      "\t\t<link\n",
      "\t\t\trel=\"search\"\n",
      "\t\t\ttype=\"application/opensearchdescription+xml\"\n",
      "\t\t\ttitle=\"Open WebUI\"\n",
      "\t\t\thref=\"/opensearch.xml\"\n",
      "\t\t/>\n",
      "\n",
      "\t\t<script>\n",
      "\t\t\tfunction resizeIframe(obj) {\n",
      "\t\t\t\tobj.style.height = obj.contentWindow.document.documentElement.scrollHeight + 'px';\n",
      "\t\t\t}\n",
      "\t\t</script>\n",
      "\n",
      "\t\t<script>\n",
      "\t\t\t// On page load or when changing themes, best to add inline in `head` to avoid FOUC\n",
      "\t\t\t(() => {\n",
      "\t\t\t\tconst metaThemeColorTag = document.querySelector('meta[name=\"theme-color\"]');\n",
      "\t\t\t\tconst prefersDarkTheme = window.matchMedia('(prefers-color-scheme: dark)').matches;\n",
      "\n",
      "\t\t\t\tif (!localStorage?.theme) {\n",
      "\t\t\t\t\tlocalStorage.theme = 'system';\n",
      "\t\t\t\t}\n",
      "\n",
      "\t\t\t\tif (localStorage.theme === 'system') {\n",
      "\t\t\t\t\tdocument.documentElement.classList.add(prefersDarkTheme ? 'dark' : 'light');\n",
      "\t\t\t\t\tmetaThemeColorTag.setAttribute('content', prefersDarkTheme ? '#171717' : '#ffffff');\n",
      "\t\t\t\t} else if (localStorage.theme === 'oled-dark') {\n",
      "\t\t\t\t\tdocument.documentElement.style.setProperty('--color-gray-800', '#101010');\n",
      "\t\t\t\t\tdocument.documentElement.style.setProperty('--color-gray-850', '#050505');\n",
      "\t\t\t\t\tdocument.documentElement.style.setProperty('--color-gray-900', '#000000');\n",
      "\t\t\t\t\tdocument.documentElement.style.setProperty('--color-gray-950', '#000000');\n",
      "\t\t\t\t\tdocument.documentElement.classList.add('dark');\n",
      "\t\t\t\t\tmetaThemeColorTag.setAttribute('content', '#000000');\n",
      "\t\t\t\t} else if (localStorage.theme === 'light') {\n",
      "\t\t\t\t\tdocument.documentElement.classList.add('light');\n",
      "\t\t\t\t\tmetaThemeColorTag.setAttribute('content', '#ffffff');\n",
      "\t\t\t\t} else if (localStorage.theme === 'her') {\n",
      "\t\t\t\t\tdocument.documentElement.classList.add('dark');\n",
      "\t\t\t\t\tdocument.documentElement.classList.add('her');\n",
      "\t\t\t\t\tmetaThemeColorTag.setAttribute('content', '#983724');\n",
      "\t\t\t\t} else {\n",
      "\t\t\t\t\tdocument.documentElement.classList.add('dark');\n",
      "\t\t\t\t\tmetaThemeColorTag.setAttribute('content', '#171717');\n",
      "\t\t\t\t}\n",
      "\n",
      "\t\t\t\twindow.matchMedia('(prefers-color-scheme: dark)').addListener((e) => {\n",
      "\t\t\t\t\tif (localStorage.theme === 'system') {\n",
      "\t\t\t\t\t\tif (e.matches) {\n",
      "\t\t\t\t\t\t\tdocument.documentElement.classList.add('dark');\n",
      "\t\t\t\t\t\t\tdocument.documentElement.classList.remove('light');\n",
      "\t\t\t\t\t\t\tmetaThemeColorTag.setAttribute('content', '#171717');\n",
      "\t\t\t\t\t\t} else {\n",
      "\t\t\t\t\t\t\tdocument.documentElement.classList.add('light');\n",
      "\t\t\t\t\t\t\tdocument.documentElement.classList.remove('dark');\n",
      "\t\t\t\t\t\t\tmetaThemeColorTag.setAttribute('content', '#ffffff');\n",
      "\t\t\t\t\t\t}\n",
      "\t\t\t\t\t}\n",
      "\t\t\t\t});\n",
      "\t\t\t})();\n",
      "\t\t</script>\n",
      "\n",
      "\t\t<title>Open WebUI</title>\n",
      "\n",
      "\t\t\n",
      "\t\t<link rel=\"modulepreload\" href=\"/_app/immutable/entry/start.DEQTVAYH.js\">\n",
      "\t\t<link rel=\"modulepreload\" href=\"/_app/immutable/chunks/entry.DXP3Dv8N.js\">\n",
      "\t\t<link rel=\"modulepreload\" href=\"/_app/immutable/chunks/scheduler.B0DTPfN8.js\">\n",
      "\t\t<link rel=\"modulepreload\" href=\"/_app/immutable/chunks/index.Dr08LSI1.js\">\n",
      "\t\t<link rel=\"modulepreload\" href=\"/_app/immutable/entry/app.BKhL4KkM.js\">\n",
      "\t\t<link rel=\"modulepreload\" href=\"/_app/immutable/chunks/preload-helper.C1FmrZbK.js\">\n",
      "\t\t<link rel=\"modulepreload\" href=\"/_app/immutable/chunks/index.CZFhs5Tc.js\">\n",
      "\t</head>\n",
      "\n",
      "\t<body data-sveltekit-preload-data=\"hover\">\n",
      "\t\t<div style=\"display: contents\">\n",
      "\t\t\t<script>\n",
      "\t\t\t\t{\n",
      "\t\t\t\t\t__sveltekit_1eyp75l = {\n",
      "\t\t\t\t\t\tbase: \"\"\n",
      "\t\t\t\t\t};\n",
      "\n",
      "\t\t\t\t\tconst element = document.currentScript.parentElement;\n",
      "\n",
      "\t\t\t\t\tPromise.all([\n",
      "\t\t\t\t\t\timport(\"/_app/immutable/entry/start.DEQTVAYH.js\"),\n",
      "\t\t\t\t\t\timport(\"/_app/immutable/entry/app.BKhL4KkM.js\")\n",
      "\t\t\t\t\t]).then(([kit, app]) => {\n",
      "\t\t\t\t\t\tkit.start(app, element);\n",
      "\t\t\t\t\t});\n",
      "\t\t\t\t}\n",
      "\t\t\t</script>\n",
      "\t\t</div>\n",
      "\n",
      "\t\t<div\n",
      "\t\t\tid=\"splash-screen\"\n",
      "\t\t\tstyle=\"position: fixed; z-index: 100; top: 0; left: 0; width: 100%; height: 100%\"\n",
      "\t\t>\n",
      "\t\t\t<style type=\"text/css\" nonce=\"\">\n",
      "\t\t\t\thtml {\n",
      "\t\t\t\t\toverflow-y: scroll !important;\n",
      "\t\t\t\t}\n",
      "\t\t\t</style>\n",
      "\n",
      "\t\t\t<img\n",
      "\t\t\t\tid=\"logo\"\n",
      "\t\t\t\tstyle=\"\n",
      "\t\t\t\t\tposition: absolute;\n",
      "\t\t\t\t\twidth: auto;\n",
      "\t\t\t\t\theight: 6rem;\n",
      "\t\t\t\t\ttop: 44%;\n",
      "\t\t\t\t\tleft: 50%;\n",
      "\t\t\t\t\ttransform: translateX(-50%);\n",
      "\t\t\t\t\"\n",
      "\t\t\t\tsrc=\"/static/splash.png\"\n",
      "\t\t\t/>\n",
      "\n",
      "\t\t\t<div\n",
      "\t\t\t\tstyle=\"\n",
      "\t\t\t\t\tposition: absolute;\n",
      "\t\t\t\t\ttop: 33%;\n",
      "\t\t\t\t\tleft: 50%;\n",
      "\n",
      "\t\t\t\t\twidth: 24rem;\n",
      "\t\t\t\t\ttransform: translateX(-50%);\n",
      "\n",
      "\t\t\t\t\tdisplay: flex;\n",
      "\t\t\t\t\tflex-direction: column;\n",
      "\t\t\t\t\talign-items: center;\n",
      "\t\t\t\t\"\n",
      "\t\t\t>\n",
      "\t\t\t\t<img\n",
      "\t\t\t\t\tid=\"logo-her\"\n",
      "\t\t\t\t\tstyle=\"width: auto; height: 13rem\"\n",
      "\t\t\t\t\tsrc=\"/static/splash.png\"\n",
      "\t\t\t\t\tclass=\"animate-pulse-fast\"\n",
      "\t\t\t\t/>\n",
      "\n",
      "\t\t\t\t<div style=\"position: relative; width: 24rem; margin-top: 0.5rem\">\n",
      "\t\t\t\t\t<div\n",
      "\t\t\t\t\t\tid=\"progress-background\"\n",
      "\t\t\t\t\t\tstyle=\"\n",
      "\t\t\t\t\t\t\tposition: absolute;\n",
      "\t\t\t\t\t\t\twidth: 100%;\n",
      "\t\t\t\t\t\t\theight: 0.75rem;\n",
      "\n",
      "\t\t\t\t\t\t\tborder-radius: 9999px;\n",
      "\t\t\t\t\t\t\tbackground-color: #fafafa9a;\n",
      "\t\t\t\t\t\t\"\n",
      "\t\t\t\t\t></div>\n",
      "\n",
      "\t\t\t\t\t<div\n",
      "\t\t\t\t\t\tid=\"progress-bar\"\n",
      "\t\t\t\t\t\tstyle=\"\n",
      "\t\t\t\t\t\t\tposition: absolute;\n",
      "\t\t\t\t\t\t\twidth: 0%;\n",
      "\t\t\t\t\t\t\theight: 0.75rem;\n",
      "\t\t\t\t\t\t\tborder-radius: 9999px;\n",
      "\t\t\t\t\t\t\tbackground-color: #fff;\n",
      "\t\t\t\t\t\t\"\n",
      "\t\t\t\t\t\tclass=\"bg-white\"\n",
      "\t\t\t\t\t></div>\n",
      "\t\t\t\t</div>\n",
      "\t\t\t</div>\n",
      "\n",
      "\t\t\t<!-- <span style=\"position: absolute; bottom: 32px; left: 50%; margin: -36px 0 0 -36px\">\n",
      "\t\t\t\tFooter content\n",
      "\t\t\t</span> -->\n",
      "\t\t</div>\n",
      "\t</body>\n",
      "</html>\n",
      "\n",
      "<style type=\"text/css\" nonce=\"\">\n",
      "\thtml {\n",
      "\t\toverflow-y: hidden !important;\n",
      "\t}\n",
      "\n",
      "\t#splash-screen {\n",
      "\t\tbackground: #fff;\n",
      "\t}\n",
      "\n",
      "\thtml.dark #splash-screen {\n",
      "\t\tbackground: #000;\n",
      "\t}\n",
      "\n",
      "\thtml.dark #splash-screen img {\n",
      "\t\tfilter: invert(1);\n",
      "\t}\n",
      "\n",
      "\thtml.her #splash-screen {\n",
      "\t\tbackground: #983724;\n",
      "\t}\n",
      "\n",
      "\t#logo-her {\n",
      "\t\tdisplay: none;\n",
      "\t}\n",
      "\n",
      "\t#progress-background {\n",
      "\t\tdisplay: none;\n",
      "\t}\n",
      "\n",
      "\t#progress-bar {\n",
      "\t\tdisplay: none;\n",
      "\t}\n",
      "\n",
      "\thtml.her #logo {\n",
      "\t\tdisplay: none;\n",
      "\t}\n",
      "\n",
      "\thtml.her #logo-her {\n",
      "\t\tdisplay: block;\n",
      "\t\tfilter: invert(1);\n",
      "\t}\n",
      "\n",
      "\thtml.her #progress-background {\n",
      "\t\tdisplay: block;\n",
      "\t}\n",
      "\n",
      "\thtml.her #progress-bar {\n",
      "\t\tdisplay: block;\n",
      "\t}\n",
      "\n",
      "\t@media (max-width: 24rem) {\n",
      "\t\thtml.her #progress-background {\n",
      "\t\t\tdisplay: none;\n",
      "\t\t}\n",
      "\n",
      "\t\thtml.her #progress-bar {\n",
      "\t\t\tdisplay: none;\n",
      "\t\t}\n",
      "\t}\n",
      "\n",
      "\t@keyframes pulse {\n",
      "\t\t50% {\n",
      "\t\t\topacity: 0.65;\n",
      "\t\t}\n",
      "\t}\n",
      "\n",
      "\t.animate-pulse-fast {\n",
      "\t\tanimation: pulse 1.5s cubic-bezier(0.4, 0, 0.6, 1) infinite;\n",
      "\t}\n",
      "</style>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "api_key = \"sk-VTukT47OBfIoc7xuogx-hg\"\n",
    "client = openai.OpenAI(\n",
    "    api_key=api_key,  # Use your LiteLLM proxy key if using virtual keys\n",
    "    base_url=\"http://0.0.0.0:4040\"  # LiteLLM proxy base URL,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': 'Budget has been exceeded! Current cost: 5.845, Max budget: 5.0', 'type': 'budget_exceeded', 'param': None, 'code': '400'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[91], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMeta_Llama_31_8b_it\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mParle moi de la seconde guerre mondiale avec un max de détails\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent)\n",
      "File \u001b[0;32m~/miniconda3/envs/llm-gateway/lib/python3.12/site-packages/openai/_utils/_utils.py:274\u001b[0m, in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    272\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    273\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 274\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/llm-gateway/lib/python3.12/site-packages/openai/resources/chat/completions.py:742\u001b[0m, in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, n, parallel_tool_calls, presence_penalty, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    546\u001b[0m \u001b[38;5;129m@overload\u001b[39m\n\u001b[1;32m    547\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    548\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    584\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    585\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m    586\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Creates a model response for the given chat conversation.\u001b[39;00m\n\u001b[1;32m    587\u001b[0m \n\u001b[1;32m    588\u001b[0m \u001b[38;5;124;03m    Learn more in the\u001b[39;00m\n\u001b[1;32m    589\u001b[0m \u001b[38;5;124;03m    [text generation](https://platform.openai.com/docs/guides/text-generation),\u001b[39;00m\n\u001b[1;32m    590\u001b[0m \u001b[38;5;124;03m    [vision](https://platform.openai.com/docs/guides/vision), and\u001b[39;00m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;124;03m    [audio](https://platform.openai.com/docs/guides/audio) guides.\u001b[39;00m\n\u001b[1;32m    592\u001b[0m \n\u001b[1;32m    593\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    594\u001b[0m \u001b[38;5;124;03m      messages: A list of messages comprising the conversation so far. Depending on the\u001b[39;00m\n\u001b[1;32m    595\u001b[0m \u001b[38;5;124;03m          [model](https://platform.openai.com/docs/models) you use, different message\u001b[39;00m\n\u001b[1;32m    596\u001b[0m \u001b[38;5;124;03m          types (modalities) are supported, like\u001b[39;00m\n\u001b[1;32m    597\u001b[0m \u001b[38;5;124;03m          [text](https://platform.openai.com/docs/guides/text-generation),\u001b[39;00m\n\u001b[1;32m    598\u001b[0m \u001b[38;5;124;03m          [images](https://platform.openai.com/docs/guides/vision), and\u001b[39;00m\n\u001b[1;32m    599\u001b[0m \u001b[38;5;124;03m          [audio](https://platform.openai.com/docs/guides/audio).\u001b[39;00m\n\u001b[1;32m    600\u001b[0m \n\u001b[1;32m    601\u001b[0m \u001b[38;5;124;03m      model: ID of the model to use. See the\u001b[39;00m\n\u001b[1;32m    602\u001b[0m \u001b[38;5;124;03m          [model endpoint compatibility](https://platform.openai.com/docs/models#model-endpoint-compatibility)\u001b[39;00m\n\u001b[1;32m    603\u001b[0m \u001b[38;5;124;03m          table for details on which models work with the Chat API.\u001b[39;00m\n\u001b[1;32m    604\u001b[0m \n\u001b[1;32m    605\u001b[0m \u001b[38;5;124;03m      stream: If set, partial message deltas will be sent, like in ChatGPT. Tokens will be\u001b[39;00m\n\u001b[1;32m    606\u001b[0m \u001b[38;5;124;03m          sent as data-only\u001b[39;00m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;124;03m          [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format)\u001b[39;00m\n\u001b[1;32m    608\u001b[0m \u001b[38;5;124;03m          as they become available, with the stream terminated by a `data: [DONE]`\u001b[39;00m\n\u001b[1;32m    609\u001b[0m \u001b[38;5;124;03m          message.\u001b[39;00m\n\u001b[1;32m    610\u001b[0m \u001b[38;5;124;03m          [Example Python code](https://cookbook.openai.com/examples/how_to_stream_completions).\u001b[39;00m\n\u001b[1;32m    611\u001b[0m \n\u001b[1;32m    612\u001b[0m \u001b[38;5;124;03m      audio: Parameters for audio output. Required when audio output is requested with\u001b[39;00m\n\u001b[1;32m    613\u001b[0m \u001b[38;5;124;03m          `modalities: [\"audio\"]`.\u001b[39;00m\n\u001b[1;32m    614\u001b[0m \u001b[38;5;124;03m          [Learn more](https://platform.openai.com/docs/guides/audio).\u001b[39;00m\n\u001b[1;32m    615\u001b[0m \n\u001b[1;32m    616\u001b[0m \u001b[38;5;124;03m      frequency_penalty: Number between -2.0 and 2.0. Positive values penalize new tokens based on their\u001b[39;00m\n\u001b[1;32m    617\u001b[0m \u001b[38;5;124;03m          existing frequency in the text so far, decreasing the model's likelihood to\u001b[39;00m\n\u001b[1;32m    618\u001b[0m \u001b[38;5;124;03m          repeat the same line verbatim.\u001b[39;00m\n\u001b[1;32m    619\u001b[0m \n\u001b[1;32m    620\u001b[0m \u001b[38;5;124;03m          [See more information about frequency and presence penalties.](https://platform.openai.com/docs/guides/text-generation)\u001b[39;00m\n\u001b[1;32m    621\u001b[0m \n\u001b[1;32m    622\u001b[0m \u001b[38;5;124;03m      function_call: Deprecated in favor of `tool_choice`.\u001b[39;00m\n\u001b[1;32m    623\u001b[0m \n\u001b[1;32m    624\u001b[0m \u001b[38;5;124;03m          Controls which (if any) function is called by the model. `none` means the model\u001b[39;00m\n\u001b[1;32m    625\u001b[0m \u001b[38;5;124;03m          will not call a function and instead generates a message. `auto` means the model\u001b[39;00m\n\u001b[1;32m    626\u001b[0m \u001b[38;5;124;03m          can pick between generating a message or calling a function. Specifying a\u001b[39;00m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;124;03m          particular function via `{\"name\": \"my_function\"}` forces the model to call that\u001b[39;00m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;124;03m          function.\u001b[39;00m\n\u001b[1;32m    629\u001b[0m \n\u001b[1;32m    630\u001b[0m \u001b[38;5;124;03m          `none` is the default when no functions are present. `auto` is the default if\u001b[39;00m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;124;03m          functions are present.\u001b[39;00m\n\u001b[1;32m    632\u001b[0m \n\u001b[1;32m    633\u001b[0m \u001b[38;5;124;03m      functions: Deprecated in favor of `tools`.\u001b[39;00m\n\u001b[1;32m    634\u001b[0m \n\u001b[1;32m    635\u001b[0m \u001b[38;5;124;03m          A list of functions the model may generate JSON inputs for.\u001b[39;00m\n\u001b[1;32m    636\u001b[0m \n\u001b[1;32m    637\u001b[0m \u001b[38;5;124;03m      logit_bias: Modify the likelihood of specified tokens appearing in the completion.\u001b[39;00m\n\u001b[1;32m    638\u001b[0m \n\u001b[1;32m    639\u001b[0m \u001b[38;5;124;03m          Accepts a JSON object that maps tokens (specified by their token ID in the\u001b[39;00m\n\u001b[1;32m    640\u001b[0m \u001b[38;5;124;03m          tokenizer) to an associated bias value from -100 to 100. Mathematically, the\u001b[39;00m\n\u001b[1;32m    641\u001b[0m \u001b[38;5;124;03m          bias is added to the logits generated by the model prior to sampling. The exact\u001b[39;00m\n\u001b[1;32m    642\u001b[0m \u001b[38;5;124;03m          effect will vary per model, but values between -1 and 1 should decrease or\u001b[39;00m\n\u001b[1;32m    643\u001b[0m \u001b[38;5;124;03m          increase likelihood of selection; values like -100 or 100 should result in a ban\u001b[39;00m\n\u001b[1;32m    644\u001b[0m \u001b[38;5;124;03m          or exclusive selection of the relevant token.\u001b[39;00m\n\u001b[1;32m    645\u001b[0m \n\u001b[1;32m    646\u001b[0m \u001b[38;5;124;03m      logprobs: Whether to return log probabilities of the output tokens or not. If true,\u001b[39;00m\n\u001b[1;32m    647\u001b[0m \u001b[38;5;124;03m          returns the log probabilities of each output token returned in the `content` of\u001b[39;00m\n\u001b[1;32m    648\u001b[0m \u001b[38;5;124;03m          `message`.\u001b[39;00m\n\u001b[1;32m    649\u001b[0m \n\u001b[1;32m    650\u001b[0m \u001b[38;5;124;03m      max_completion_tokens: An upper bound for the number of tokens that can be generated for a completion,\u001b[39;00m\n\u001b[1;32m    651\u001b[0m \u001b[38;5;124;03m          including visible output tokens and\u001b[39;00m\n\u001b[1;32m    652\u001b[0m \u001b[38;5;124;03m          [reasoning tokens](https://platform.openai.com/docs/guides/reasoning).\u001b[39;00m\n\u001b[1;32m    653\u001b[0m \n\u001b[1;32m    654\u001b[0m \u001b[38;5;124;03m      max_tokens: The maximum number of [tokens](/tokenizer) that can be generated in the chat\u001b[39;00m\n\u001b[1;32m    655\u001b[0m \u001b[38;5;124;03m          completion. This value can be used to control\u001b[39;00m\n\u001b[1;32m    656\u001b[0m \u001b[38;5;124;03m          [costs](https://openai.com/api/pricing/) for text generated via API.\u001b[39;00m\n\u001b[1;32m    657\u001b[0m \n\u001b[1;32m    658\u001b[0m \u001b[38;5;124;03m          This value is now deprecated in favor of `max_completion_tokens`, and is not\u001b[39;00m\n\u001b[1;32m    659\u001b[0m \u001b[38;5;124;03m          compatible with\u001b[39;00m\n\u001b[1;32m    660\u001b[0m \u001b[38;5;124;03m          [o1 series models](https://platform.openai.com/docs/guides/reasoning).\u001b[39;00m\n\u001b[1;32m    661\u001b[0m \n\u001b[1;32m    662\u001b[0m \u001b[38;5;124;03m      metadata: Developer-defined tags and values used for filtering completions in the\u001b[39;00m\n\u001b[1;32m    663\u001b[0m \u001b[38;5;124;03m          [dashboard](https://platform.openai.com/chat-completions).\u001b[39;00m\n\u001b[1;32m    664\u001b[0m \n\u001b[1;32m    665\u001b[0m \u001b[38;5;124;03m      modalities: Output types that you would like the model to generate for this request. Most\u001b[39;00m\n\u001b[1;32m    666\u001b[0m \u001b[38;5;124;03m          models are capable of generating text, which is the default:\u001b[39;00m\n\u001b[1;32m    667\u001b[0m \n\u001b[1;32m    668\u001b[0m \u001b[38;5;124;03m          `[\"text\"]`\u001b[39;00m\n\u001b[1;32m    669\u001b[0m \n\u001b[1;32m    670\u001b[0m \u001b[38;5;124;03m          The `gpt-4o-audio-preview` model can also be used to\u001b[39;00m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;124;03m          [generate audio](https://platform.openai.com/docs/guides/audio). To request that\u001b[39;00m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;124;03m          this model generate both text and audio responses, you can use:\u001b[39;00m\n\u001b[1;32m    673\u001b[0m \n\u001b[1;32m    674\u001b[0m \u001b[38;5;124;03m          `[\"text\", \"audio\"]`\u001b[39;00m\n\u001b[1;32m    675\u001b[0m \n\u001b[1;32m    676\u001b[0m \u001b[38;5;124;03m      n: How many chat completion choices to generate for each input message. Note that\u001b[39;00m\n\u001b[1;32m    677\u001b[0m \u001b[38;5;124;03m          you will be charged based on the number of generated tokens across all of the\u001b[39;00m\n\u001b[1;32m    678\u001b[0m \u001b[38;5;124;03m          choices. Keep `n` as `1` to minimize costs.\u001b[39;00m\n\u001b[1;32m    679\u001b[0m \n\u001b[1;32m    680\u001b[0m \u001b[38;5;124;03m      parallel_tool_calls: Whether to enable\u001b[39;00m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;124;03m          [parallel function calling](https://platform.openai.com/docs/guides/function-calling#configuring-parallel-function-calling)\u001b[39;00m\n\u001b[1;32m    682\u001b[0m \u001b[38;5;124;03m          during tool use.\u001b[39;00m\n\u001b[1;32m    683\u001b[0m \n\u001b[1;32m    684\u001b[0m \u001b[38;5;124;03m      prediction: Static predicted output content, such as the content of a text file that is\u001b[39;00m\n\u001b[1;32m    685\u001b[0m \u001b[38;5;124;03m          being regenerated.\u001b[39;00m\n\u001b[1;32m    686\u001b[0m \n\u001b[1;32m    687\u001b[0m \u001b[38;5;124;03m      presence_penalty: Number between -2.0 and 2.0. Positive values penalize new tokens based on\u001b[39;00m\n\u001b[1;32m    688\u001b[0m \u001b[38;5;124;03m          whether they appear in the text so far, increasing the model's likelihood to\u001b[39;00m\n\u001b[1;32m    689\u001b[0m \u001b[38;5;124;03m          talk about new topics.\u001b[39;00m\n\u001b[1;32m    690\u001b[0m \n\u001b[1;32m    691\u001b[0m \u001b[38;5;124;03m          [See more information about frequency and presence penalties.](https://platform.openai.com/docs/guides/text-generation)\u001b[39;00m\n\u001b[1;32m    692\u001b[0m \n\u001b[1;32m    693\u001b[0m \u001b[38;5;124;03m      response_format: An object specifying the format that the model must output. Compatible with\u001b[39;00m\n\u001b[1;32m    694\u001b[0m \u001b[38;5;124;03m          [GPT-4o](https://platform.openai.com/docs/models#gpt-4o),\u001b[39;00m\n\u001b[1;32m    695\u001b[0m \u001b[38;5;124;03m          [GPT-4o mini](https://platform.openai.com/docs/models#gpt-4o-mini),\u001b[39;00m\n\u001b[1;32m    696\u001b[0m \u001b[38;5;124;03m          [GPT-4 Turbo](https://platform.openai.com/docs/models#gpt-4-turbo-and-gpt-4) and\u001b[39;00m\n\u001b[1;32m    697\u001b[0m \u001b[38;5;124;03m          all GPT-3.5 Turbo models newer than `gpt-3.5-turbo-1106`.\u001b[39;00m\n\u001b[1;32m    698\u001b[0m \n\u001b[1;32m    699\u001b[0m \u001b[38;5;124;03m          Setting to `{ \"type\": \"json_schema\", \"json_schema\": {...} }` enables Structured\u001b[39;00m\n\u001b[1;32m    700\u001b[0m \u001b[38;5;124;03m          Outputs which ensures the model will match your supplied JSON schema. Learn more\u001b[39;00m\n\u001b[1;32m    701\u001b[0m \u001b[38;5;124;03m          in the\u001b[39;00m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;124;03m          [Structured Outputs guide](https://platform.openai.com/docs/guides/structured-outputs).\u001b[39;00m\n\u001b[1;32m    703\u001b[0m \n\u001b[1;32m    704\u001b[0m \u001b[38;5;124;03m          Setting to `{ \"type\": \"json_object\" }` enables JSON mode, which ensures the\u001b[39;00m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;124;03m          message the model generates is valid JSON.\u001b[39;00m\n\u001b[1;32m    706\u001b[0m \n\u001b[1;32m    707\u001b[0m \u001b[38;5;124;03m          **Important:** when using JSON mode, you **must** also instruct the model to\u001b[39;00m\n\u001b[1;32m    708\u001b[0m \u001b[38;5;124;03m          produce JSON yourself via a system or user message. Without this, the model may\u001b[39;00m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;124;03m          generate an unending stream of whitespace until the generation reaches the token\u001b[39;00m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;124;03m          limit, resulting in a long-running and seemingly \"stuck\" request. Also note that\u001b[39;00m\n\u001b[1;32m    711\u001b[0m \u001b[38;5;124;03m          the message content may be partially cut off if `finish_reason=\"length\"`, which\u001b[39;00m\n\u001b[1;32m    712\u001b[0m \u001b[38;5;124;03m          indicates the generation exceeded `max_tokens` or the conversation exceeded the\u001b[39;00m\n\u001b[1;32m    713\u001b[0m \u001b[38;5;124;03m          max context length.\u001b[39;00m\n\u001b[1;32m    714\u001b[0m \n\u001b[1;32m    715\u001b[0m \u001b[38;5;124;03m      seed: This feature is in Beta. If specified, our system will make a best effort to\u001b[39;00m\n\u001b[1;32m    716\u001b[0m \u001b[38;5;124;03m          sample deterministically, such that repeated requests with the same `seed` and\u001b[39;00m\n\u001b[1;32m    717\u001b[0m \u001b[38;5;124;03m          parameters should return the same result. Determinism is not guaranteed, and you\u001b[39;00m\n\u001b[1;32m    718\u001b[0m \u001b[38;5;124;03m          should refer to the `system_fingerprint` response parameter to monitor changes\u001b[39;00m\n\u001b[1;32m    719\u001b[0m \u001b[38;5;124;03m          in the backend.\u001b[39;00m\n\u001b[1;32m    720\u001b[0m \n\u001b[1;32m    721\u001b[0m \u001b[38;5;124;03m      service_tier: Specifies the latency tier to use for processing the request. This parameter is\u001b[39;00m\n\u001b[1;32m    722\u001b[0m \u001b[38;5;124;03m          relevant for customers subscribed to the scale tier service:\u001b[39;00m\n\u001b[1;32m    723\u001b[0m \n\u001b[1;32m    724\u001b[0m \u001b[38;5;124;03m          - If set to 'auto', and the Project is Scale tier enabled, the system will\u001b[39;00m\n\u001b[1;32m    725\u001b[0m \u001b[38;5;124;03m            utilize scale tier credits until they are exhausted.\u001b[39;00m\n\u001b[1;32m    726\u001b[0m \u001b[38;5;124;03m          - If set to 'auto', and the Project is not Scale tier enabled, the request will\u001b[39;00m\n\u001b[1;32m    727\u001b[0m \u001b[38;5;124;03m            be processed using the default service tier with a lower uptime SLA and no\u001b[39;00m\n\u001b[1;32m    728\u001b[0m \u001b[38;5;124;03m            latency guarentee.\u001b[39;00m\n\u001b[1;32m    729\u001b[0m \u001b[38;5;124;03m          - If set to 'default', the request will be processed using the default service\u001b[39;00m\n\u001b[1;32m    730\u001b[0m \u001b[38;5;124;03m            tier with a lower uptime SLA and no latency guarentee.\u001b[39;00m\n\u001b[1;32m    731\u001b[0m \u001b[38;5;124;03m          - When not set, the default behavior is 'auto'.\u001b[39;00m\n\u001b[1;32m    732\u001b[0m \n\u001b[1;32m    733\u001b[0m \u001b[38;5;124;03m          When this parameter is set, the response body will include the `service_tier`\u001b[39;00m\n\u001b[1;32m    734\u001b[0m \u001b[38;5;124;03m          utilized.\u001b[39;00m\n\u001b[1;32m    735\u001b[0m \n\u001b[1;32m    736\u001b[0m \u001b[38;5;124;03m      stop: Up to 4 sequences where the API will stop generating further tokens.\u001b[39;00m\n\u001b[1;32m    737\u001b[0m \n\u001b[1;32m    738\u001b[0m \u001b[38;5;124;03m      store: Whether or not to store the output of this chat completion request for use in\u001b[39;00m\n\u001b[1;32m    739\u001b[0m \u001b[38;5;124;03m          our [model distillation](https://platform.openai.com/docs/guides/distillation)\u001b[39;00m\n\u001b[1;32m    740\u001b[0m \u001b[38;5;124;03m          or [evals](https://platform.openai.com/docs/guides/evals) products.\u001b[39;00m\n\u001b[1;32m    741\u001b[0m \n\u001b[0;32m--> 742\u001b[0m \u001b[38;5;124;03m      stream_options: Options for streaming response. Only set this when you set `stream: true`.\u001b[39;00m\n\u001b[1;32m    743\u001b[0m \n\u001b[1;32m    744\u001b[0m \u001b[38;5;124;03m      temperature: What sampling temperature to use, between 0 and 2. Higher values like 0.8 will\u001b[39;00m\n\u001b[1;32m    745\u001b[0m \u001b[38;5;124;03m          make the output more random, while lower values like 0.2 will make it more\u001b[39;00m\n\u001b[1;32m    746\u001b[0m \u001b[38;5;124;03m          focused and deterministic.\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \n\u001b[1;32m    748\u001b[0m \u001b[38;5;124;03m          We generally recommend altering this or `top_p` but not both.\u001b[39;00m\n\u001b[1;32m    749\u001b[0m \n\u001b[1;32m    750\u001b[0m \u001b[38;5;124;03m      tool_choice: Controls which (if any) tool is called by the model. `none` means the model will\u001b[39;00m\n\u001b[1;32m    751\u001b[0m \u001b[38;5;124;03m          not call any tool and instead generates a message. `auto` means the model can\u001b[39;00m\n\u001b[1;32m    752\u001b[0m \u001b[38;5;124;03m          pick between generating a message or calling one or more tools. `required` means\u001b[39;00m\n\u001b[1;32m    753\u001b[0m \u001b[38;5;124;03m          the model must call one or more tools. Specifying a particular tool via\u001b[39;00m\n\u001b[1;32m    754\u001b[0m \u001b[38;5;124;03m          `{\"type\": \"function\", \"function\": {\"name\": \"my_function\"}}` forces the model to\u001b[39;00m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;124;03m          call that tool.\u001b[39;00m\n\u001b[1;32m    756\u001b[0m \n\u001b[1;32m    757\u001b[0m \u001b[38;5;124;03m          `none` is the default when no tools are present. `auto` is the default if tools\u001b[39;00m\n\u001b[1;32m    758\u001b[0m \u001b[38;5;124;03m          are present.\u001b[39;00m\n\u001b[1;32m    759\u001b[0m \n\u001b[1;32m    760\u001b[0m \u001b[38;5;124;03m      tools: A list of tools the model may call. Currently, only functions are supported as a\u001b[39;00m\n\u001b[1;32m    761\u001b[0m \u001b[38;5;124;03m          tool. Use this to provide a list of functions the model may generate JSON inputs\u001b[39;00m\n\u001b[1;32m    762\u001b[0m \u001b[38;5;124;03m          for. A max of 128 functions are supported.\u001b[39;00m\n\u001b[1;32m    763\u001b[0m \n\u001b[1;32m    764\u001b[0m \u001b[38;5;124;03m      top_logprobs: An integer between 0 and 20 specifying the number of most likely tokens to\u001b[39;00m\n\u001b[1;32m    765\u001b[0m \u001b[38;5;124;03m          return at each token position, each with an associated log probability.\u001b[39;00m\n\u001b[1;32m    766\u001b[0m \u001b[38;5;124;03m          `logprobs` must be set to `true` if this parameter is used.\u001b[39;00m\n\u001b[1;32m    767\u001b[0m \n\u001b[1;32m    768\u001b[0m \u001b[38;5;124;03m      top_p: An alternative to sampling with temperature, called nucleus sampling, where the\u001b[39;00m\n\u001b[1;32m    769\u001b[0m \u001b[38;5;124;03m          model considers the results of the tokens with top_p probability mass. So 0.1\u001b[39;00m\n\u001b[1;32m    770\u001b[0m \u001b[38;5;124;03m          means only the tokens comprising the top 10% probability mass are considered.\u001b[39;00m\n\u001b[1;32m    771\u001b[0m \n\u001b[1;32m    772\u001b[0m \u001b[38;5;124;03m          We generally recommend altering this or `temperature` but not both.\u001b[39;00m\n\u001b[1;32m    773\u001b[0m \n\u001b[1;32m    774\u001b[0m \u001b[38;5;124;03m      user: A unique identifier representing your end-user, which can help OpenAI to monitor\u001b[39;00m\n\u001b[1;32m    775\u001b[0m \u001b[38;5;124;03m          and detect abuse.\u001b[39;00m\n\u001b[1;32m    776\u001b[0m \u001b[38;5;124;03m          [Learn more](https://platform.openai.com/docs/guides/safety-best-practices#end-user-ids).\u001b[39;00m\n\u001b[1;32m    777\u001b[0m \n\u001b[1;32m    778\u001b[0m \u001b[38;5;124;03m      extra_headers: Send extra headers\u001b[39;00m\n\u001b[1;32m    779\u001b[0m \n\u001b[1;32m    780\u001b[0m \u001b[38;5;124;03m      extra_query: Add additional query parameters to the request\u001b[39;00m\n\u001b[1;32m    781\u001b[0m \n\u001b[1;32m    782\u001b[0m \u001b[38;5;124;03m      extra_body: Add additional JSON properties to the request\u001b[39;00m\n\u001b[1;32m    783\u001b[0m \n\u001b[1;32m    784\u001b[0m \u001b[38;5;124;03m      timeout: Override the client-level default timeout for this request, in seconds\u001b[39;00m\n\u001b[1;32m    785\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    786\u001b[0m     \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/llm-gateway/lib/python3.12/site-packages/openai/_base_client.py:1277\u001b[0m, in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1266\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1267\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1268\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1275\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1276\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m-> 1277\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1278\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1279\u001b[0m     )\n\u001b[1;32m   1280\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls))\n",
      "File \u001b[0;32m~/miniconda3/envs/llm-gateway/lib/python3.12/site-packages/openai/_base_client.py:954\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    952\u001b[0m if remaining_retries is not None:\n\u001b[1;32m    953\u001b[0m     retries_taken = options.get_max_retries(self.max_retries) - remaining_retries\n\u001b[0;32m--> 954\u001b[0m else:\n\u001b[1;32m    955\u001b[0m     retries_taken = 0\n\u001b[1;32m    957\u001b[0m return self._request(\n\u001b[1;32m    958\u001b[0m     cast_to=cast_to,\n\u001b[1;32m    959\u001b[0m     options=options,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    962\u001b[0m     retries_taken=retries_taken,\n\u001b[1;32m    963\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/llm-gateway/lib/python3.12/site-packages/openai/_base_client.py:1058\u001b[0m, in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1055\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m   1056\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m   1057\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n\u001b[0;32m-> 1058\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1060\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': 'Budget has been exceeded! Current cost: 5.845, Max budget: 5.0', 'type': 'budget_exceeded', 'param': None, 'code': '400'}}"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"Meta_Llama_31_8b_it\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Parle moi de la seconde guerre mondiale avec un max de détails\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from litellm import token_counter\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": \"Hey, how's it going\"}]\n",
    "token_count = token_counter(model=\"gpt-3.5-turbo\", messages=messages)\n",
    "print(f\"Token count: {token_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lucie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La Révolution française de 1789 fut un événement majeur de l'histoire de France et du monde. Elle a commencé avec la prise de la Bastille le 14 juillet 1789 et s'est achevée avec la proclamation de la Déclaration des droits de l'Homme et du Citoyen le 26 août 1789.\n",
      "\n",
      "La Révolution française a eu de nombreuses causes, telles que la montée de l'absolutisme royal, la crise financière et économique, l'inégalité sociale et les tensions politiques. Elle a également été influencée par les idées des Lumières et de la philosophie radicale.\n",
      "\n",
      "La Révolution française a entraîné de nombreux changements, notamment la fin de la monarchie absolue et l'établissement de la République. Elle a également conduit à la Déclaration des droits de l'Homme et du Citoyen, qui a établi des principes fondamentaux tels que la liberté, l'égalité et la fraternité.\n",
      "\n",
      "Cependant, la Révolution française a également été marquée par la violence et la terreur, avec l'exécution de nombreux nobles et la mise en place de la Terreur par Robespierre.\n",
      "\n",
      "La Révolution française a eu un impact majeur sur l'histoire de France et du monde, et continue d'être étudiée et discutée à ce jour.\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"Lucie_7B\",\n",
    "    temperature=0.6,\n",
    "    user='user3@maif.com',\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Parle moi de la révolution française\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chat-58e6116d091c4f06b7f59a6148f97225', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"La Révolution française de 1789 fut un événement majeur de l'histoire de France et du monde. Elle a commencé avec la prise de la Bastille le 14 juillet 1789 et s'est achevée avec la proclamation de la Déclaration des droits de l'Homme et du Citoyen le 26 août 1789.\\n\\nLa Révolution française a eu de nombreuses causes, telles que la montée de l'absolutisme royal, la crise financière et économique, l'inégalité sociale et les tensions politiques. Elle a également été influencée par les idées des Lumières et de la philosophie radicale.\\n\\nLa Révolution française a entraîné de nombreux changements, notamment la fin de la monarchie absolue et l'établissement de la République. Elle a également conduit à la Déclaration des droits de l'Homme et du Citoyen, qui a établi des principes fondamentaux tels que la liberté, l'égalité et la fraternité.\\n\\nCependant, la Révolution française a également été marquée par la violence et la terreur, avec l'exécution de nombreux nobles et la mise en place de la Terreur par Robespierre.\\n\\nLa Révolution française a eu un impact majeur sur l'histoire de France et du monde, et continue d'être étudiée et discutée à ce jour.\", refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1733754016, model='lucie-7b', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=258, prompt_tokens=17, total_tokens=275, completion_tokens_details=None, prompt_tokens_details=None))\n"
     ]
    }
   ],
   "source": [
    "response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curl -X GET 'http://localhost:4000/global/spend/report?start_date=2024-12-01&end_date=2024-12-31&group_by=team' \\\n",
    "  -H 'Authorization: Bearer sk-1234'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "url = 'http://localhost:4000/global/spend/report'\n",
    "params = {\n",
    "    'start_date': '2024-12-01',\n",
    "    'end_date': '2024-12-30'\n",
    "}\n",
    "\n",
    "headers = {\n",
    "    'Authorization': 'Bearer sk-1234'\n",
    "}\n",
    "\n",
    "# Make the GET request\n",
    "response = requests.get(url, headers=headers, params=params)\n",
    "spend_report = response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Get list of models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"data\":[{\"id\":\"Meta_Llama_31_8b_it\",\"object\":\"model\",\"created\":1677610602,\"owned_by\":\"openai\"},{\"id\":\"Lucie_7B\",\"object\":\"model\",\"created\":1677610602,\"owned_by\":\"openai\"}],\"object\":\"list\"}"
     ]
    }
   ],
   "source": [
    "!curl -X 'GET' \\\n",
    "  'http://localhost:4000/v1/models' \\\n",
    "  -H 'accept: application/json' \\\n",
    "  -H 'Authorization: Bearer sk-1234'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Generate key via api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<string>, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m<string>:10\u001b[0;36m\u001b[0m\n\u001b[0;31m    }'\u001b[0m\n\u001b[0m      ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "!curl -X 'POST' \\\n",
    "  'http://localhost:4000/key/generate' \\\n",
    "  -H 'accept: application/json' \\\n",
    "  -H 'Content-Type: application/json' \\\n",
    "  -H 'Authorization: Bearer sk-1234' \\\n",
    "  -d '{\n",
    "    \"key_alias\": \"Sales\",\n",
    "    \"models\": [\"Lucie_7B\"],\n",
    "    \"max_budget\": 10,\n",
    "    \"user_id\": \"user1_id\",\n",
    "    \"team_id\": \"team1_id\",\n",
    "    \"max_parallel_requests\": 10,\n",
    "    \"blocked\": true,\n",
    "    \"aliases\": {}\n",
    "  }'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"keys\":[\"88dc28d0f030c55ed4ab77ed8faf098196cb1c05df778539800c9f1243fe6b4b\",\"983002003de2c682d18c86f0f450f9b3469d88021e7a8e01bb28ba46adeff48a\",\"d1117f612d7a35a93e69584ff2d595aa603e598e980dd4d1fd9640b5f884b1f2\"],\"total_count\":3,\"current_page\":1,\"total_pages\":1}curl: (6) Could not resolve host: \\\n"
     ]
    }
   ],
   "source": [
    "!curl -X 'GET' \\\n",
    "  'http://localhost:4000/key/list?page=1&size=10' \\\n",
    "  -H 'accept: application/json' \\\n",
    "  -H 'Authorization: Bearer sk-1234' \\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]"
     ]
    }
   ],
   "source": [
    "!curl -X GET \"http://0.0.0.0:4000/spend/tags\" \\\n",
    "-H \"Authorization: Bearer sk-1234\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"error\":{\"message\":\"Authentication Error, No api key passed in.\",\"type\":\"auth_error\",\"param\":\"None\",\"code\":\"401\"}}"
     ]
    }
   ],
   "source": [
    "!curl -X 'GET' \\\n",
    "  'http://localhost:4000/global/spend/report?start_date=2024-12-02&end_date=2024-12-10&group_by=MAIF&api_key=sk-1234&internal_user_id=user1' \\\n",
    "  -H 'accept: application/json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-gateway",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
