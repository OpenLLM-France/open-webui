{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening the authorization URL in your browser: http://163.114.159.68:8001/application/o/authorize/?client_id=rVzfdONVVWKEw3T601zp9sdyAVa6l4XUtTRh7bH9&redirect_uri=http%3A%2F%2F163.114.159.68%3A8080%2Foauth%2Foidc%2Fcallback&response_type=code&scope=openid+email+profile&state=test_state\n",
      "After you approve the app, paste the redirect URL here:\n",
      "Received redirect response: http://163.114.159.68:8080/oauth/oidc/callback?code=30cdeccebfa449aa8949d116a8a667f9&state=test_state\n",
      "Received Authorization Code: 30cdeccebfa449aa8949d116a8a667f9\n",
      "Access Token: eyJhbGciOiJSU0EtT0FFUC0yNTYiLCJlbmMiOiJBMjU2Q0JDLUhTNTEyIiwia2lkIjoiMTAzZTQyNzUwNjc5NzdhY2VmZDQ5ZmM1YWFhMTEyMmUiLCJ0eXAiOiJKV0UifQ.N9M_gOe5MU68SxJjAQbm8Umu1vQF9HMyHFZrXI3q3jdHbqI0LeJNC4kTmNEm-ECdjHJkCSQvvqBZjRDkNWuvdjb5g5Y1kAtSB2j7MgZH2UozhTzNwkaEa-mHwGPd1DIecvEC96huRrA1hwGFE4V88bT5SWh_TIYtu1xNltacjx1U2cPOiYhA3C8q2BWBjbWhV-JBYvLJ46pEAg37npAlhCf24Aoa1Y9Xr-0eBIPIzO5tjFYOBAZbCgJ3lPrP-7bvKGEL0yp__TPisrNv9YNDU-pWv_LTT9B3Ua9tUd-k9xNuw4N88q-07YhavUkVVDNii3UTObyH0d0VIe9RaR4opl6S4VL_wI7ukSXG_iBJ10coO8mdN2PQGpabuQ_x4sQ90wbrZYS7AH64gFgCZg8_hkdiI-Fw-Z9Va2-lnPmOf9-DZ9eo34O9JAknna9EOIOZSOz2OU62xXzggeDjLuu4OQHHeR1dGP1eaTY0qhliL44-ShvmHc4g7JFjFiN9gFggtjn1SE8Zt86iCloJ6PfLI-Y0dFqHkVupZjGmkfi8hMxfKVLT1Fqjv6nMu3wkIdFTa7_pRBVFlPaRttpNk4VcmTMTnfAIFTRuRWM8Zeb5StWVOzBRoXOOME-FQSHbP7JTmFb-OYWwMZ5br2Sn8YD8Rvv6pdsOKc98pvzxvSNPwzw.Gq0Dfj3LOPFnyFcusUQ2lw.ta0GZZua-LF4Lz76XywS94G_zHfrCOj3SPyHMNOtfCCEvAyLzgswVNNg6ekvB_sUtDQyXKIgOufVF1wYju3rBq3kk3T-UFEH-M5iBXNU2JwiTDmelp0X_l1rpoRXc0fYPKsYt20iaM7SF8rf6dyBf_c1nV6RAOQ48w8afsI-md_s6D_Pppe5y7cyYQ4jfIj0pl_E1yuYQs6ml-_ik3TrPoNpIziy2Vrl7MhYERKmvFoK_fP4L7_Nyr4QSWOQLE1bcxRLfHvEjW38-Lww3ujL71UrOZFkuuAQp7LfRKrszLnjb7GU-ekV2Bwbqljcr7Lov5MsajK0JMq1wbfekEIo5ytuUmYbDTwqrtTvO-RP8g2Td9DV4UvvQ74yau8Jf2UOvOAxaXq_s9qYFImA_xNt9lTO1gJ7wj9kA0eEJOp-KZv-NYn875X3g5SKTrTWkUKdmqjLVqPSI8Ur3uwz5dwduL-_Aw-hTHMmFXCZDkXXFgxUZrbgEffhgvbl-XIqOled3GKGg5SEKJ_qvcAx6KUHze6daqSqdlKnSmHeoacBGofSKk8QlC-7pOyLk7fgzOsvwdGku3JETcfMvNwZKWvaM0ZCZxO9VZRI7JOVI6FpUB0CsydhrOXBLwDrs9WqQ3Q1n6LQcyYkTbe1H0-Z4wMaMjI-0QUHVaJcp-jGV6Si0TGWFL1OyPCN7nmqxj5SDPjjq8yQG_q7Bi_MlRgIH8SXbFypjnRQxGt0N5rsptmM9CDe5xvC3GSP8r2d7CmTIfLmsMUd3HM8CTXOillfQOLRKus3JM1cyalL-KcSq38DNSbH6aWfHIs6LbSXnStkvCallmuIQxSs7BMsR8bgizgz7X1NL3yfbe1CG6-Ay-Kvw_AyqO_dGCjrOKffRXpasFFqN4zTtFzdlEVBVFezaeRYvOgy8M-t-DCuJlm4hmR5z43344uF0J5NxSSikJSfMvVeFoG6kJpttjM2oADgTVc3AZnNVL9svHVbTAs1szNFXfGkS4Tvt9x4oOqhgGXAIcYzGID9e0xtZrkTZ_L9PVkDuYKvJ6SiGTpRdiYUt-zAOedWCMNd9PM9qYerUJs_F2VavAt1wc1XATqqh-T4AJuFzkqEZsZgRmFxCy0I3IlhMZWviSc-GfBPaQvJUpXS4gGwsbHDSm_XTMMOQlsO2nYEgdhubVKgpbLuyA4-rT-hVnX07vJ4eOMDwuBdf2LyrfK6BE1q-cES2w25KBktgGszsRWRkTw9IR6M0LR2jLZpNpS_bX5hhlfbA3SlUvmqeW8-3GfDRjir73YOJvvDhG8GM85e5BVhq86iPfFqbMTldwCbuoZa9ECXZ9SJivUXqzxZjekC9Kh_DeJLH05OQw9P9DNFYFDOWW86h4t__aP34Ip7JniIsyn7BIwpa-Sk-xTRaGi-CmPX7vtr6qHvN3n1GeFM5Rq6OG7EEmjJBD4hJNYpjc55A2V50GgV1lqHMahPCUeo53rmZMB4r8CowADG8fiqb-uBLqM1hFk09HtA3k6Yapp9IzWOXlyMQqWEEPKGMDzfCfXnVTabBHYuh1yBy7e-JuJlTCBc4XpKUvBKKmRzjra-y4cZR0tZMITWvGbhUhV8VA8pyq60j38_Rqz4dAKOsTplReELk7x92I-z05Kywu-Ru7na7fKHWwSJZNryIKubaHdTPyCNcdNur1TzpDiDLg90EGE4akZBvJuAwi6Awwaq1gOixYihd0zG7sZGF5biM268HPNfKpnrFbKaQ9m99r1y72X9asQ4h50J6th4Eb5L3FdZzfOf4HAseWDWgr6AVMEIhQ4VqBhfOfP-MesbO9fbmekanXO8uBonRFCUsJ299A6litG-3EhZDgSQatzi8ExsMkGvsS0fHmYyFC73k_cHV5ajmH0-KZwuNp7RQbUy6Gmv95MXgkL3ZZhoetxDQ6lO8g168UoFsChT_VZxDn-5NuxZF6702OtDbPBOqfTN0J7MVuJdkRzZC6qbeQR5CnAX4edBea6lzhL63lJmmu_16T7eDmv8q3RNbhbRKIUtWjsF8gFZmpcUjCiRdKzaqLZzNfQ9XQBXd0-w9SG3c_XjtEneFyrkbfr4X8mHqOOfHJlaiFAyqpgUy6QrpgGuoL5RQnhCr1uPj81kJASS1rQesyiyYNHrYbRd7NuLSMcUs4Fu153k7Y7Om-up1iMTJYASjcxoZBwmeIZat2g1ICLENU-r4Ub5oIkuAU6jO5Y1eW0huszONGZBwjrAAYPiZbdjq5NiK6GM2bg1fA.FHZjFX3-kR420MHvp4VBmBp_fQ0sy5LyW4KA0afUVRY\n",
      "ID Token: eyJhbGciOiJSU0EtT0FFUC0yNTYiLCJlbmMiOiJBMjU2Q0JDLUhTNTEyIiwia2lkIjoiMTAzZTQyNzUwNjc5NzdhY2VmZDQ5ZmM1YWFhMTEyMmUiLCJ0eXAiOiJKV0UifQ.mt2R6UUHgbcE9T-SQ9ewkn4lFbDqnKItJY1NnKjDkf60m6qzobLpHWbsLhoW1MZK8KKWcKYKWV_NQcGUZmaMm4MTapYgR8ZNMuQzRw5JIVWbeFYdiIn6A73DQSdEuzaPtWwwBnBQWg6iI_nfjAMqLVD4FJvBwBcAj-yvZaWatXbGoy35UAy8YIWqs0Tc4X2jlLM6ewAL_2oNypGa22ZL3lwV_racbRuUpo-a2_0wiexs9kZoo4fo4plnq7HJtuY85FABKAT4Ion5Z3mW6CIJ9TRyRfo_kNIzpfmWBtyJui14yYHR-kXzhunZlkr4CTsYmJOj_cbwz0SrnHYKchN06JxM7PTzPQowbTEFkjcyaY0p27KpeH53L1Zid9MxprnEHBec7R6DMhNKStI3vYDcZwjqI71kqoLZB4zpeyXn_ECB05AS2nqRJh9PfKEz6qYaSAwK3NiYJKvFtSiyDR2JCn0yyuR6KSdeLEU1bMTUhmh7F1vAw0eI8lKfs-7vGGEPFeRpgv1DHFn0F_eN6EQkAdxJs4rEXy-cqjfBU37v-hK_Yu_2v_2yVmlDVH8Z5_ogUL8sYYUHvqIbSS559eB3Cv69FerXm2D3dcXo6DSKW9nzDSo3AHll05g8ApsYR87PjMN-Fs4aPshcKQ6HRLbyGBofVa2Rm3HPsn9GdXhZ61s.6uLnqwzcrnssQfDCLcVlsQ.mbS12h7qsy2TO0Rz9gtngnc8yoJEYY5MuYxnBa5fm2zlCUNS5wsE9-TYSP_kXprxONKykAaZY3HmVZuRrNnmAZEyBYoViBgjvRkdObiQfXnrUgQ3t-AS3CUMkSHmTB729dxMLhkAZnezmxFFBHiW-6KAJngF475fBLyaYv78BZMceA5B_PsA1g5rjBD3CTosxuomcB9iYvQ7HBoc7KZJrqPfLhvVrqzWyNhc2h-qUdMf4mAxSfsVr3hXhI5prXVugSo76MoVGh7qZSQEFBm3432DWaYXOdIJoMOP9ADit323J6GrtA1g51w5ztn9YkTDkPybveA53oTxVX42vwlJNz8fHwKmRy5bgdRckLeL8oEgx2dPV0XM2NZkgSgsekOxZ7DPKsk8WxzEbBguyYadfbIwjYkHpNLlM3pduhS7ZhxUrPtRjGqGCzA8yJotwAXdZUY42OAnxxJ0bYiJaEO0l1wCebGBM6C3gisd4BkL0wPzFmBWmWVO4xIydn4B3ePlschWAeg8DWJPJZBUhm6CR7FPkyH9GZMANFYggkm0BLJ8_EmIhY0F3RMHmSBVhhpOKWGdKjD9uP_Wu2xv_rRJkWfg3SeSIG8gat1BhkCKzDx1RARKDdH1siHRcChcz_7majSO8JwqnJJdNfvgiZFkj_rEEM2BIzvKAfkzfhvOxeJ4LEqq3U5jKd4TPMXUwZkB0erDDdye3o5p2VG31wKHv2kM3yl_CXSSNziMz2X8-Lxrhe4RTKOJ7r0A84J28UaLVeu1K9j66zIIKHvzbaEX5v_mCK1J4JjxQdgVmdRqY1mqcUJgRTsLEjQgVPQpyZKeLxLK_jiSZu8w4V_Y3NjNHKe85uIJu8ntZ0U4U6VRV-QUwW8DF2Ui4TeJfkEUmdy2-cqeo3oAsUVdRxm7_2GEymxyf9o-8WxWmgIxM76QxVy4RqbVdAOYXxn2l-7ginv14BN8nAIx8FeI8OtOE3jGw9aVXe6_FPmMKROPPC8NHsmJ5921qtuIoCkwP7XtQA6CDqaKDwE7BZRYX_bqyVZzTrv2R8kVavFU0T9i9eSNPClApKLsHJpfhA1XyC2Nj3I1lfAH_v8sLNC-ZLUFmua0z8dDDbn8zuhfU_axPft_ObDiCNAaPZjkDM3LPlp704ihArl0muEvrETEfwyKb-nyI5Gd2TVfbV2gUF73G58DklkXdgva35wptcuY-IeBuPOyRC5YnO_c_xEfyBpiWuzr_9sni1zmb9WUU6xUakAdD13DHlzZPGdb_TJ02xTevXwVea-tR4Ba7TbXtJdJThGRTymM1Ehb0Z1CiqCSluO29kFftN3lZ5-o27m0fU0HtmqRnPAQJDLQk5uOZzpqqvEuslXm949iDnNFq4K7hB5876nrH8CsCtkt_2nznFyf4qP4AiOM5m2ZbvULNOHT9uMYwWzDugFcw6CExk4T0nhBhT2O1W3PGlelMpI50l4oqhH0-xX3sXFzBRZjZcQG9l79fTk-0pOKzl6lV4M3cLmIf0dn6lFewvZssynekvwkNw8hFLiZznHo4vN8iSGACuURXKNz6qMrKm1M3_qxQ-AczGZh2n_nDUBvXYD6VfhgmtZyc-lYTJgXZDDs0W-pvi5Gyn9Y4z5rD_cV9xBbSDFX9_kC7rvDYItq1pe-uyBLqE9OMahOx4d41bp3tKUIIo7vDyJcjWgJrCwqMnWXk6DU6ImfELD423gszVRx9ANUltDzXhEJ2R5LbeTAmZbPR2HWkPu70tDX02cyeJSpcM8V5XBfS93wuPcnQQiL-_XCBjEhIXeyRRVtZyCsq15i3o1ZhfWTzrV1moWNXF1lO_VV7e3ZjIg1zOukW1pL0sJynZZdviL2zlejCgzsi6gcHarISCyTN_U3AUhU2Ir30uFGAXZ3a9Om3MlIgWQLU92TGMClGSo-limeG-oXO9Cxb4KQu72RBeLJhfXsjfIF0alcvXhx-2TIvntzkE5mW9ZPyNxpjIV49TGhkplLwS3uG8Y8e1IkY46VGyq9mZK_yHScurEyW5zoVJlOxlSNEBin9SsklMCRbJXYYBXbaruOu8A8fZJ0ZnuRWXlQSM7dvFAskTg.i6wTCRpY6Zgq6n5Hz2Tu74x70xv-ptXoaFL7VF9sv7s\n",
      "Refresh Token: None\n",
      "UserInfo: {'sub': '49c341926c1ad3bb5aaa90bb5c90a65f896e8347540b940463087971232eceb0', 'email': 'htagourti@linagora.com', 'email_verified': True, 'name': 'authentik Default Admin', 'given_name': 'authentik Default Admin', 'preferred_username': 'akadmin', 'nickname': 'akadmin', 'groups': ['authentik Admins']}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from urllib.parse import urlencode, urlparse, parse_qs\n",
    "import webbrowser\n",
    "import time\n",
    "\n",
    "# Replace with your Authentik configuration\n",
    "AUTHENTIK_CONFIG = {\n",
    "    \"issuer\": \"http://163.114.159.68:8001/application/o/open-webui/\",\n",
    "    \"authorization_endpoint\": \"http://163.114.159.68:8001/application/o/authorize/\",\n",
    "    \"token_endpoint\": \"http://163.114.159.68:8001/application/o/token/\",\n",
    "    \"userinfo_endpoint\": \"http://163.114.159.68:8001/application/o/userinfo/\",\n",
    "    \"client_id\": \"rVzfdONVVWKEw3T601zp9sdyAVa6l4XUtTRh7bH9\",          # Replace with your client ID\n",
    "    \"client_secret\": \"Zaxc9q7n8RCsJgNy728AuxRqpyAYsobbfukHIaNOUVa4EyntM10KfdpqyYVx1ZMeboEyXpjiYG72zX3erG2WczFWytle8ssZlh1ox9JnM1in7mmfKMHAaePZASc30qLB\",  # Replace with your client secret\n",
    "    \"redirect_uri\": \"http://163.114.159.68:8080/oauth/oidc/callback\",  # Your redirect URI\n",
    "    \"scope\": \"openid email profile\",  # Requested scopes\n",
    "}\n",
    "\n",
    "# Step 1: Generate the Authorization URL\n",
    "def generate_auth_url():\n",
    "    params = {\n",
    "        \"client_id\": AUTHENTIK_CONFIG[\"client_id\"],\n",
    "        \"redirect_uri\": AUTHENTIK_CONFIG[\"redirect_uri\"],\n",
    "        \"response_type\": \"code\",  # Authorization Code Grant\n",
    "        \"scope\": AUTHENTIK_CONFIG[\"scope\"],\n",
    "        \"state\": \"test_state\",  # Random state for CSRF protection\n",
    "    }\n",
    "    url = f\"{AUTHENTIK_CONFIG['authorization_endpoint']}?{urlencode(params)}\"\n",
    "    return url\n",
    "\n",
    "# Step 2: Simulate the User Authorization Flow\n",
    "def get_auth_code():\n",
    "    auth_url = generate_auth_url()\n",
    "    print(f\"Opening the authorization URL in your browser: {auth_url}\")\n",
    "    time.sleep(1)  # To make the print visible\n",
    "    webbrowser.open(auth_url)\n",
    "    print(\"After you approve the app, paste the redirect URL here:\")\n",
    "    redirect_response = input(\"Redirect URL: \").strip()\n",
    "    print(f\"Received redirect response: {redirect_response}\")\n",
    "    # Extract the \"code\" and \"state\" from the redirect URL\n",
    "    parsed_url = urlparse(redirect_response)\n",
    "    query_params = parse_qs(parsed_url.query)\n",
    "    auth_code = query_params.get(\"code\", [None])[0]\n",
    "    state = query_params.get(\"state\", [None])[0]\n",
    "    \n",
    "    if not auth_code or not state:\n",
    "        raise Exception(\"Failed to get authorization code or state.\")\n",
    "    \n",
    "    print(f\"Received Authorization Code: {auth_code}\")\n",
    "    return auth_code\n",
    "\n",
    "# Step 3: Exchange Authorization Code for Tokens\n",
    "def exchange_code_for_tokens(auth_code):\n",
    "    token_endpoint = AUTHENTIK_CONFIG[\"token_endpoint\"]\n",
    "    data = {\n",
    "        \"grant_type\": \"authorization_code\",\n",
    "        \"code\": auth_code,\n",
    "        \"redirect_uri\": AUTHENTIK_CONFIG[\"redirect_uri\"],\n",
    "        \"client_id\": AUTHENTIK_CONFIG[\"client_id\"],\n",
    "        \"client_secret\": AUTHENTIK_CONFIG[\"client_secret\"],\n",
    "    }\n",
    "    response = requests.post(token_endpoint, data=data)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Failed to retrieve tokens: {response.text}\")\n",
    "    \n",
    "    tokens = response.json()\n",
    "    print(f\"Access Token: {tokens.get('access_token')}\")\n",
    "    print(f\"ID Token: {tokens.get('id_token')}\")\n",
    "    print(f\"Refresh Token: {tokens.get('refresh_token')}\")\n",
    "    return tokens\n",
    "\n",
    "# Step 4: Retrieve UserInfo using Access Token\n",
    "def get_userinfo(access_token):\n",
    "    userinfo_endpoint = AUTHENTIK_CONFIG[\"userinfo_endpoint\"]\n",
    "    headers = {\"Authorization\": f\"Bearer {access_token}\"}\n",
    "    response = requests.get(userinfo_endpoint, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Failed to retrieve userinfo: {response.text}\")\n",
    "    \n",
    "    userinfo = response.json()\n",
    "    print(f\"UserInfo: {userinfo}\")\n",
    "    return userinfo\n",
    "\n",
    "\n",
    "# Step 1: User Authorization\n",
    "auth_code = get_auth_code()\n",
    "\n",
    "# Step 2: Exchange Authorization Code for Tokens\n",
    "tokens = exchange_code_for_tokens(auth_code)\n",
    "access_token = tokens.get(\"access_token\")\n",
    "\n",
    "# Step 3: Retrieve UserInfo\n",
    "if access_token:\n",
    "    get_userinfo(access_token)\n",
    "else:\n",
    "    print(\"Access token not found.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from authlib.integrations.starlette_client import OAuth\n",
    "OAUTH_PROVIDERS= {'oidc': {'client_id': 'rVzfdONVVWKEw3T601zp9sdyAVa6l4XUtTRh7bH9', 'client_secret': 'Zaxc9q7n8RCsJgNy728AuxRqpyAYsobbfukHIaNOUVa4EyntM10KfdpqyYVx1ZMeboEyXpjiYG72zX3erG2WczFWytle8ssZlh1ox9JnM1in7mmfKMHAaePZASc30qLB', 'server_metadata_url': 'http://163.114.159.68:8001/application/o/open-webui/.well-known/openid-configuration', 'scope': 'openid email profile', 'name': 'authentik', 'redirect_uri': 'http://163.114.159.68:8080/oauth/oidc/callback'}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "oauth = OAuth()\n",
    "for provider_name, provider_config in OAUTH_PROVIDERS.items():\n",
    "    oauth.register(\n",
    "        name=provider_name,\n",
    "        client_id=provider_config[\"client_id\"],\n",
    "        client_secret=provider_config[\"client_secret\"],\n",
    "        server_metadata_url=provider_config[\"server_metadata_url\"],\n",
    "        client_kwargs={\n",
    "            \"scope\": provider_config[\"scope\"],\n",
    "        },\n",
    "        redirect_uri=provider_config[\"redirect_uri\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = oauth.create_client('oidc')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import Request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class d:\n",
    "    def __init__ (self,query_params):\n",
    "        self.query_params = query_params\n",
    "        self.session={}\n",
    "        self.redirect_uri=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = {'code': '4659985056fd4a05a96376c51481e81b', 'state': 'vFVwjKRQdbyRBB6BrkZITQIGmUCOY3'}\n",
    "request = d(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "OAuthError",
     "evalue": "invalid_grant: The provided authorization grant or refresh token is invalid, expired, revoked, does not match the redirection URI used in the authorization request, or was issued to another client",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOAuthError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m client\u001b[38;5;241m.\u001b[39mauthorize_access_token(request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m~/miniconda3/envs/pegasus/lib/python3.12/site-packages/authlib/integrations/starlette_client/apps.py:82\u001b[0m, in \u001b[0;36mStarletteOAuth2App.authorize_access_token\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m#await self.framework.clear_state_data(session, params.get('state'))\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m#params = self._format_state_params(state_data, params)\u001b[39;00m\n\u001b[1;32m     81\u001b[0m params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mredirect_uri\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp://163.114.159.68:8080/oauth/oidc/callback\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 82\u001b[0m token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfetch_access_token(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid_token\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m token \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnonce\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m state_data:\n\u001b[1;32m     85\u001b[0m     userinfo \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_id_token(token, nonce\u001b[38;5;241m=\u001b[39mstate_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnonce\u001b[39m\u001b[38;5;124m'\u001b[39m], claims_options\u001b[38;5;241m=\u001b[39mclaims_options)\n",
      "File \u001b[0;32m~/miniconda3/envs/pegasus/lib/python3.12/site-packages/authlib/integrations/base_client/async_app.py:125\u001b[0m, in \u001b[0;36mAsyncOAuth2Mixin.fetch_access_token\u001b[0;34m(self, redirect_uri, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m         params\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccess_token_params)\n\u001b[1;32m    124\u001b[0m     params\u001b[38;5;241m.\u001b[39mupdate(kwargs)\n\u001b[0;32m--> 125\u001b[0m     token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m client\u001b[38;5;241m.\u001b[39mfetch_token(token_endpoint, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m token\n",
      "File \u001b[0;32m~/miniconda3/envs/pegasus/lib/python3.12/site-packages/authlib/integrations/httpx_client/oauth2_client.py:138\u001b[0m, in \u001b[0;36mAsyncOAuth2Client._fetch_token\u001b[0;34m(self, url, body, headers, auth, method, **kwargs)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompliance_hook[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccess_token_response\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m    136\u001b[0m     resp \u001b[38;5;241m=\u001b[39m hook(resp)\n\u001b[0;32m--> 138\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_response_token\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresp\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pegasus/lib/python3.12/site-packages/authlib/oauth2/client.py:355\u001b[0m, in \u001b[0;36mOAuth2Client.parse_response_token\u001b[0;34m(self, resp)\u001b[0m\n\u001b[1;32m    353\u001b[0m token \u001b[38;5;241m=\u001b[39m resp\u001b[38;5;241m.\u001b[39mjson()\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m token:\n\u001b[0;32m--> 355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moauth_error_class(\n\u001b[1;32m    356\u001b[0m         error\u001b[38;5;241m=\u001b[39mtoken[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    357\u001b[0m         description\u001b[38;5;241m=\u001b[39mtoken\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124merror_description\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    358\u001b[0m     )\n\u001b[1;32m    359\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken \u001b[38;5;241m=\u001b[39m token\n\u001b[1;32m    360\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken\n",
      "\u001b[0;31mOAuthError\u001b[0m: invalid_grant: The provided authorization grant or refresh token is invalid, expired, revoked, does not match the redirection URI used in the authorization request, or was issued to another client"
     ]
    }
   ],
   "source": [
    "await client.authorize_access_token(request=request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stripe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "{'key': 'sk-VTukT47OBfIoc7xuogx-hg', 'info': {'key_name': 'sk-...x-hg', 'key_alias': 'test-key', 'soft_budget_cooldown': False, 'spend': 9.876271, 'expires': None, 'models': ['all-team-models'], 'aliases': {}, 'config': {}, 'user_id': 'user-15', 'team_id': None, 'permissions': {}, 'max_parallel_requests': None, 'metadata': {}, 'blocked': None, 'tpm_limit': None, 'rpm_limit': None, 'max_budget': 10.0, 'budget_duration': None, 'budget_reset_at': None, 'allowed_cache_controls': [], 'model_spend': {}, 'model_max_budget': {}, 'budget_id': None, 'created_at': '2024-12-11T08:57:17.779000+00:00', 'updated_at': '2024-12-13T10:02:49.005000+00:00', 'litellm_budget_table': None}}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = \"http://localhost:4040/key/info\"\n",
    "key = \"sk-VTukT47OBfIoc7xuogx-hg\"\n",
    "auth_token = \"sk-1234\"\n",
    "\n",
    "# Constructing the headers\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {auth_token}\"\n",
    "}\n",
    "\n",
    "# Sending the GET request\n",
    "response = requests.get(url, params={\"key\": key}, headers=headers)\n",
    "\n",
    "# Printing the response\n",
    "print(response.status_code)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'key': 'sk-VTukT47OBfIoc7xuogx-hg',\n",
       " 'info': {'key_name': 'sk-...x-hg',\n",
       "  'key_alias': 'test-key',\n",
       "  'soft_budget_cooldown': False,\n",
       "  'spend': 9.876271,\n",
       "  'expires': None,\n",
       "  'models': ['all-team-models'],\n",
       "  'aliases': {},\n",
       "  'config': {},\n",
       "  'user_id': 'user-15',\n",
       "  'team_id': None,\n",
       "  'permissions': {},\n",
       "  'max_parallel_requests': None,\n",
       "  'metadata': {},\n",
       "  'blocked': None,\n",
       "  'tpm_limit': None,\n",
       "  'rpm_limit': None,\n",
       "  'max_budget': 10.0,\n",
       "  'budget_duration': None,\n",
       "  'budget_reset_at': None,\n",
       "  'allowed_cache_controls': [],\n",
       "  'model_spend': {},\n",
       "  'model_max_budget': {},\n",
       "  'budget_id': None,\n",
       "  'created_at': '2024-12-11T08:57:17.779000+00:00',\n",
       "  'updated_at': '2024-12-13T10:02:49.005000+00:00',\n",
       "  'litellm_budget_table': None}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "{'key': 'sk-VTukT47OBfIoc7xuogx-hg', 'token': 'c9f39d62231e157a4dc466d0fee0695860ab364cd52e979e7707306db1b0fb85', 'key_name': 'sk-...x-hg', 'key_alias': 'test-key', 'soft_budget_cooldown': False, 'spend': 9.0, 'expires': None, 'models': ['all-team-models'], 'aliases': {}, 'config': {}, 'user_id': 'user-15', 'team_id': None, 'permissions': {}, 'max_parallel_requests': None, 'metadata': {}, 'blocked': None, 'tpm_limit': None, 'rpm_limit': None, 'max_budget': 10.0, 'budget_duration': None, 'budget_reset_at': None, 'allowed_cache_controls': [], 'model_spend': {}, 'model_max_budget': {}, 'budget_id': None, 'created_at': '2024-12-11T08:57:17.779000+00:00', 'updated_at': '2024-12-13T10:00:29.291000+00:00', 'litellm_budget_table': None}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "url = \"http://localhost:4040/key/update\"\n",
    "auth_token = \"sk-1234\"\n",
    "\n",
    "# Constructing headers\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {auth_token}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "# Data to be sent in the POST request\n",
    "data = {\n",
    "    \"key\": \"sk-VTukT47OBfIoc7xuogx-hg\",\n",
    "    \"user_id\": \"user-15\",\n",
    "    \"max_budget\": 10,\n",
    "    \"spend\" : 9\n",
    "}\n",
    "\n",
    "# Sending the POST request\n",
    "response = requests.post(url, headers=headers, data=json.dumps(data))\n",
    "\n",
    "# Printing the response\n",
    "print(response.status_code)\n",
    "print(response.json())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400\n",
      "{'detail': 'Invalid signature'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = 'http://localhost:8000/stripe/webhook'\n",
    "\n",
    "\n",
    "headers = {\n",
    "    'Content-Type': 'application/json'\n",
    "}\n",
    "\n",
    "response = requests.post(url, json=payload, headers=headers)\n",
    "print(response.status_code)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'{\"message\":\"Hello\"}'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'http://localhost:8000/stripe/test'\n",
    "response = requests.get(url)\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!doctype html>\n",
      "<html lang=\"en\">\n",
      "\t<head>\n",
      "\t\t<meta charset=\"utf-8\" />\n",
      "\t\t<link rel=\"icon\" type=\"image/png\" href=\"/favicon/favicon-96x96.png\" sizes=\"96x96\" />\n",
      "\t\t<link rel=\"icon\" type=\"image/svg+xml\" href=\"/favicon/favicon.svg\" />\n",
      "\t\t<link rel=\"shortcut icon\" href=\"/favicon/favicon.ico\" />\n",
      "\t\t<link rel=\"apple-touch-icon\" sizes=\"180x180\" href=\"/favicon/apple-touch-icon.png\" />\n",
      "\t\t<meta name=\"apple-mobile-web-app-title\" content=\"Open WebUI\" />\n",
      "\t\t<link rel=\"manifest\" href=\"/favicon/site.webmanifest\" />\n",
      "\t\t<meta\n",
      "\t\t\tname=\"viewport\"\n",
      "\t\t\tcontent=\"width=device-width, initial-scale=1, maximum-scale=1, viewport-fit=cover\"\n",
      "\t\t/>\n",
      "\t\t<meta name=\"theme-color\" content=\"#171717\" />\n",
      "\t\t<meta name=\"robots\" content=\"noindex,nofollow\" />\n",
      "\t\t<meta name=\"description\" content=\"Open WebUI\" />\n",
      "\t\t<link\n",
      "\t\t\trel=\"search\"\n",
      "\t\t\ttype=\"application/opensearchdescription+xml\"\n",
      "\t\t\ttitle=\"Open WebUI\"\n",
      "\t\t\thref=\"/opensearch.xml\"\n",
      "\t\t/>\n",
      "\n",
      "\t\t<script>\n",
      "\t\t\tfunction resizeIframe(obj) {\n",
      "\t\t\t\tobj.style.height = obj.contentWindow.document.documentElement.scrollHeight + 'px';\n",
      "\t\t\t}\n",
      "\t\t</script>\n",
      "\n",
      "\t\t<script>\n",
      "\t\t\t// On page load or when changing themes, best to add inline in `head` to avoid FOUC\n",
      "\t\t\t(() => {\n",
      "\t\t\t\tconst metaThemeColorTag = document.querySelector('meta[name=\"theme-color\"]');\n",
      "\t\t\t\tconst prefersDarkTheme = window.matchMedia('(prefers-color-scheme: dark)').matches;\n",
      "\n",
      "\t\t\t\tif (!localStorage?.theme) {\n",
      "\t\t\t\t\tlocalStorage.theme = 'system';\n",
      "\t\t\t\t}\n",
      "\n",
      "\t\t\t\tif (localStorage.theme === 'system') {\n",
      "\t\t\t\t\tdocument.documentElement.classList.add(prefersDarkTheme ? 'dark' : 'light');\n",
      "\t\t\t\t\tmetaThemeColorTag.setAttribute('content', prefersDarkTheme ? '#171717' : '#ffffff');\n",
      "\t\t\t\t} else if (localStorage.theme === 'oled-dark') {\n",
      "\t\t\t\t\tdocument.documentElement.style.setProperty('--color-gray-800', '#101010');\n",
      "\t\t\t\t\tdocument.documentElement.style.setProperty('--color-gray-850', '#050505');\n",
      "\t\t\t\t\tdocument.documentElement.style.setProperty('--color-gray-900', '#000000');\n",
      "\t\t\t\t\tdocument.documentElement.style.setProperty('--color-gray-950', '#000000');\n",
      "\t\t\t\t\tdocument.documentElement.classList.add('dark');\n",
      "\t\t\t\t\tmetaThemeColorTag.setAttribute('content', '#000000');\n",
      "\t\t\t\t} else if (localStorage.theme === 'light') {\n",
      "\t\t\t\t\tdocument.documentElement.classList.add('light');\n",
      "\t\t\t\t\tmetaThemeColorTag.setAttribute('content', '#ffffff');\n",
      "\t\t\t\t} else if (localStorage.theme === 'her') {\n",
      "\t\t\t\t\tdocument.documentElement.classList.add('dark');\n",
      "\t\t\t\t\tdocument.documentElement.classList.add('her');\n",
      "\t\t\t\t\tmetaThemeColorTag.setAttribute('content', '#983724');\n",
      "\t\t\t\t} else {\n",
      "\t\t\t\t\tdocument.documentElement.classList.add('dark');\n",
      "\t\t\t\t\tmetaThemeColorTag.setAttribute('content', '#171717');\n",
      "\t\t\t\t}\n",
      "\n",
      "\t\t\t\twindow.matchMedia('(prefers-color-scheme: dark)').addListener((e) => {\n",
      "\t\t\t\t\tif (localStorage.theme === 'system') {\n",
      "\t\t\t\t\t\tif (e.matches) {\n",
      "\t\t\t\t\t\t\tdocument.documentElement.classList.add('dark');\n",
      "\t\t\t\t\t\t\tdocument.documentElement.classList.remove('light');\n",
      "\t\t\t\t\t\t\tmetaThemeColorTag.setAttribute('content', '#171717');\n",
      "\t\t\t\t\t\t} else {\n",
      "\t\t\t\t\t\t\tdocument.documentElement.classList.add('light');\n",
      "\t\t\t\t\t\t\tdocument.documentElement.classList.remove('dark');\n",
      "\t\t\t\t\t\t\tmetaThemeColorTag.setAttribute('content', '#ffffff');\n",
      "\t\t\t\t\t\t}\n",
      "\t\t\t\t\t}\n",
      "\t\t\t\t});\n",
      "\t\t\t})();\n",
      "\t\t</script>\n",
      "\n",
      "\t\t<title>Open WebUI</title>\n",
      "\n",
      "\t\t\n",
      "\t\t<link rel=\"modulepreload\" href=\"/_app/immutable/entry/start.DEQTVAYH.js\">\n",
      "\t\t<link rel=\"modulepreload\" href=\"/_app/immutable/chunks/entry.DXP3Dv8N.js\">\n",
      "\t\t<link rel=\"modulepreload\" href=\"/_app/immutable/chunks/scheduler.B0DTPfN8.js\">\n",
      "\t\t<link rel=\"modulepreload\" href=\"/_app/immutable/chunks/index.Dr08LSI1.js\">\n",
      "\t\t<link rel=\"modulepreload\" href=\"/_app/immutable/entry/app.BKhL4KkM.js\">\n",
      "\t\t<link rel=\"modulepreload\" href=\"/_app/immutable/chunks/preload-helper.C1FmrZbK.js\">\n",
      "\t\t<link rel=\"modulepreload\" href=\"/_app/immutable/chunks/index.CZFhs5Tc.js\">\n",
      "\t</head>\n",
      "\n",
      "\t<body data-sveltekit-preload-data=\"hover\">\n",
      "\t\t<div style=\"display: contents\">\n",
      "\t\t\t<script>\n",
      "\t\t\t\t{\n",
      "\t\t\t\t\t__sveltekit_1eyp75l = {\n",
      "\t\t\t\t\t\tbase: \"\"\n",
      "\t\t\t\t\t};\n",
      "\n",
      "\t\t\t\t\tconst element = document.currentScript.parentElement;\n",
      "\n",
      "\t\t\t\t\tPromise.all([\n",
      "\t\t\t\t\t\timport(\"/_app/immutable/entry/start.DEQTVAYH.js\"),\n",
      "\t\t\t\t\t\timport(\"/_app/immutable/entry/app.BKhL4KkM.js\")\n",
      "\t\t\t\t\t]).then(([kit, app]) => {\n",
      "\t\t\t\t\t\tkit.start(app, element);\n",
      "\t\t\t\t\t});\n",
      "\t\t\t\t}\n",
      "\t\t\t</script>\n",
      "\t\t</div>\n",
      "\n",
      "\t\t<div\n",
      "\t\t\tid=\"splash-screen\"\n",
      "\t\t\tstyle=\"position: fixed; z-index: 100; top: 0; left: 0; width: 100%; height: 100%\"\n",
      "\t\t>\n",
      "\t\t\t<style type=\"text/css\" nonce=\"\">\n",
      "\t\t\t\thtml {\n",
      "\t\t\t\t\toverflow-y: scroll !important;\n",
      "\t\t\t\t}\n",
      "\t\t\t</style>\n",
      "\n",
      "\t\t\t<img\n",
      "\t\t\t\tid=\"logo\"\n",
      "\t\t\t\tstyle=\"\n",
      "\t\t\t\t\tposition: absolute;\n",
      "\t\t\t\t\twidth: auto;\n",
      "\t\t\t\t\theight: 6rem;\n",
      "\t\t\t\t\ttop: 44%;\n",
      "\t\t\t\t\tleft: 50%;\n",
      "\t\t\t\t\ttransform: translateX(-50%);\n",
      "\t\t\t\t\"\n",
      "\t\t\t\tsrc=\"/static/splash.png\"\n",
      "\t\t\t/>\n",
      "\n",
      "\t\t\t<div\n",
      "\t\t\t\tstyle=\"\n",
      "\t\t\t\t\tposition: absolute;\n",
      "\t\t\t\t\ttop: 33%;\n",
      "\t\t\t\t\tleft: 50%;\n",
      "\n",
      "\t\t\t\t\twidth: 24rem;\n",
      "\t\t\t\t\ttransform: translateX(-50%);\n",
      "\n",
      "\t\t\t\t\tdisplay: flex;\n",
      "\t\t\t\t\tflex-direction: column;\n",
      "\t\t\t\t\talign-items: center;\n",
      "\t\t\t\t\"\n",
      "\t\t\t>\n",
      "\t\t\t\t<img\n",
      "\t\t\t\t\tid=\"logo-her\"\n",
      "\t\t\t\t\tstyle=\"width: auto; height: 13rem\"\n",
      "\t\t\t\t\tsrc=\"/static/splash.png\"\n",
      "\t\t\t\t\tclass=\"animate-pulse-fast\"\n",
      "\t\t\t\t/>\n",
      "\n",
      "\t\t\t\t<div style=\"position: relative; width: 24rem; margin-top: 0.5rem\">\n",
      "\t\t\t\t\t<div\n",
      "\t\t\t\t\t\tid=\"progress-background\"\n",
      "\t\t\t\t\t\tstyle=\"\n",
      "\t\t\t\t\t\t\tposition: absolute;\n",
      "\t\t\t\t\t\t\twidth: 100%;\n",
      "\t\t\t\t\t\t\theight: 0.75rem;\n",
      "\n",
      "\t\t\t\t\t\t\tborder-radius: 9999px;\n",
      "\t\t\t\t\t\t\tbackground-color: #fafafa9a;\n",
      "\t\t\t\t\t\t\"\n",
      "\t\t\t\t\t></div>\n",
      "\n",
      "\t\t\t\t\t<div\n",
      "\t\t\t\t\t\tid=\"progress-bar\"\n",
      "\t\t\t\t\t\tstyle=\"\n",
      "\t\t\t\t\t\t\tposition: absolute;\n",
      "\t\t\t\t\t\t\twidth: 0%;\n",
      "\t\t\t\t\t\t\theight: 0.75rem;\n",
      "\t\t\t\t\t\t\tborder-radius: 9999px;\n",
      "\t\t\t\t\t\t\tbackground-color: #fff;\n",
      "\t\t\t\t\t\t\"\n",
      "\t\t\t\t\t\tclass=\"bg-white\"\n",
      "\t\t\t\t\t></div>\n",
      "\t\t\t\t</div>\n",
      "\t\t\t</div>\n",
      "\n",
      "\t\t\t<!-- <span style=\"position: absolute; bottom: 32px; left: 50%; margin: -36px 0 0 -36px\">\n",
      "\t\t\t\tFooter content\n",
      "\t\t\t</span> -->\n",
      "\t\t</div>\n",
      "\t</body>\n",
      "</html>\n",
      "\n",
      "<style type=\"text/css\" nonce=\"\">\n",
      "\thtml {\n",
      "\t\toverflow-y: hidden !important;\n",
      "\t}\n",
      "\n",
      "\t#splash-screen {\n",
      "\t\tbackground: #fff;\n",
      "\t}\n",
      "\n",
      "\thtml.dark #splash-screen {\n",
      "\t\tbackground: #000;\n",
      "\t}\n",
      "\n",
      "\thtml.dark #splash-screen img {\n",
      "\t\tfilter: invert(1);\n",
      "\t}\n",
      "\n",
      "\thtml.her #splash-screen {\n",
      "\t\tbackground: #983724;\n",
      "\t}\n",
      "\n",
      "\t#logo-her {\n",
      "\t\tdisplay: none;\n",
      "\t}\n",
      "\n",
      "\t#progress-background {\n",
      "\t\tdisplay: none;\n",
      "\t}\n",
      "\n",
      "\t#progress-bar {\n",
      "\t\tdisplay: none;\n",
      "\t}\n",
      "\n",
      "\thtml.her #logo {\n",
      "\t\tdisplay: none;\n",
      "\t}\n",
      "\n",
      "\thtml.her #logo-her {\n",
      "\t\tdisplay: block;\n",
      "\t\tfilter: invert(1);\n",
      "\t}\n",
      "\n",
      "\thtml.her #progress-background {\n",
      "\t\tdisplay: block;\n",
      "\t}\n",
      "\n",
      "\thtml.her #progress-bar {\n",
      "\t\tdisplay: block;\n",
      "\t}\n",
      "\n",
      "\t@media (max-width: 24rem) {\n",
      "\t\thtml.her #progress-background {\n",
      "\t\t\tdisplay: none;\n",
      "\t\t}\n",
      "\n",
      "\t\thtml.her #progress-bar {\n",
      "\t\t\tdisplay: none;\n",
      "\t\t}\n",
      "\t}\n",
      "\n",
      "\t@keyframes pulse {\n",
      "\t\t50% {\n",
      "\t\t\topacity: 0.65;\n",
      "\t\t}\n",
      "\t}\n",
      "\n",
      "\t.animate-pulse-fast {\n",
      "\t\tanimation: pulse 1.5s cubic-bezier(0.4, 0, 0.6, 1) infinite;\n",
      "\t}\n",
      "</style>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "api_key = \"sk-VTukT47OBfIoc7xuogx-hg\"\n",
    "client = openai.OpenAI(\n",
    "    api_key=api_key,  # Use your LiteLLM proxy key if using virtual keys\n",
    "    base_url=\"http://0.0.0.0:4040\"  # LiteLLM proxy base URL,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': 'Budget has been exceeded! Current cost: 5.845, Max budget: 5.0', 'type': 'budget_exceeded', 'param': None, 'code': '400'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[91], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMeta_Llama_31_8b_it\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mParle moi de la seconde guerre mondiale avec un max de dÃ©tails\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent)\n",
      "File \u001b[0;32m~/miniconda3/envs/llm-gateway/lib/python3.12/site-packages/openai/_utils/_utils.py:274\u001b[0m, in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    272\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    273\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 274\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/llm-gateway/lib/python3.12/site-packages/openai/resources/chat/completions.py:742\u001b[0m, in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, n, parallel_tool_calls, presence_penalty, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    546\u001b[0m \u001b[38;5;129m@overload\u001b[39m\n\u001b[1;32m    547\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    548\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    584\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    585\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m    586\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Creates a model response for the given chat conversation.\u001b[39;00m\n\u001b[1;32m    587\u001b[0m \n\u001b[1;32m    588\u001b[0m \u001b[38;5;124;03m    Learn more in the\u001b[39;00m\n\u001b[1;32m    589\u001b[0m \u001b[38;5;124;03m    [text generation](https://platform.openai.com/docs/guides/text-generation),\u001b[39;00m\n\u001b[1;32m    590\u001b[0m \u001b[38;5;124;03m    [vision](https://platform.openai.com/docs/guides/vision), and\u001b[39;00m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;124;03m    [audio](https://platform.openai.com/docs/guides/audio) guides.\u001b[39;00m\n\u001b[1;32m    592\u001b[0m \n\u001b[1;32m    593\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    594\u001b[0m \u001b[38;5;124;03m      messages: A list of messages comprising the conversation so far. Depending on the\u001b[39;00m\n\u001b[1;32m    595\u001b[0m \u001b[38;5;124;03m          [model](https://platform.openai.com/docs/models) you use, different message\u001b[39;00m\n\u001b[1;32m    596\u001b[0m \u001b[38;5;124;03m          types (modalities) are supported, like\u001b[39;00m\n\u001b[1;32m    597\u001b[0m \u001b[38;5;124;03m          [text](https://platform.openai.com/docs/guides/text-generation),\u001b[39;00m\n\u001b[1;32m    598\u001b[0m \u001b[38;5;124;03m          [images](https://platform.openai.com/docs/guides/vision), and\u001b[39;00m\n\u001b[1;32m    599\u001b[0m \u001b[38;5;124;03m          [audio](https://platform.openai.com/docs/guides/audio).\u001b[39;00m\n\u001b[1;32m    600\u001b[0m \n\u001b[1;32m    601\u001b[0m \u001b[38;5;124;03m      model: ID of the model to use. See the\u001b[39;00m\n\u001b[1;32m    602\u001b[0m \u001b[38;5;124;03m          [model endpoint compatibility](https://platform.openai.com/docs/models#model-endpoint-compatibility)\u001b[39;00m\n\u001b[1;32m    603\u001b[0m \u001b[38;5;124;03m          table for details on which models work with the Chat API.\u001b[39;00m\n\u001b[1;32m    604\u001b[0m \n\u001b[1;32m    605\u001b[0m \u001b[38;5;124;03m      stream: If set, partial message deltas will be sent, like in ChatGPT. Tokens will be\u001b[39;00m\n\u001b[1;32m    606\u001b[0m \u001b[38;5;124;03m          sent as data-only\u001b[39;00m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;124;03m          [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format)\u001b[39;00m\n\u001b[1;32m    608\u001b[0m \u001b[38;5;124;03m          as they become available, with the stream terminated by a `data: [DONE]`\u001b[39;00m\n\u001b[1;32m    609\u001b[0m \u001b[38;5;124;03m          message.\u001b[39;00m\n\u001b[1;32m    610\u001b[0m \u001b[38;5;124;03m          [Example Python code](https://cookbook.openai.com/examples/how_to_stream_completions).\u001b[39;00m\n\u001b[1;32m    611\u001b[0m \n\u001b[1;32m    612\u001b[0m \u001b[38;5;124;03m      audio: Parameters for audio output. Required when audio output is requested with\u001b[39;00m\n\u001b[1;32m    613\u001b[0m \u001b[38;5;124;03m          `modalities: [\"audio\"]`.\u001b[39;00m\n\u001b[1;32m    614\u001b[0m \u001b[38;5;124;03m          [Learn more](https://platform.openai.com/docs/guides/audio).\u001b[39;00m\n\u001b[1;32m    615\u001b[0m \n\u001b[1;32m    616\u001b[0m \u001b[38;5;124;03m      frequency_penalty: Number between -2.0 and 2.0. Positive values penalize new tokens based on their\u001b[39;00m\n\u001b[1;32m    617\u001b[0m \u001b[38;5;124;03m          existing frequency in the text so far, decreasing the model's likelihood to\u001b[39;00m\n\u001b[1;32m    618\u001b[0m \u001b[38;5;124;03m          repeat the same line verbatim.\u001b[39;00m\n\u001b[1;32m    619\u001b[0m \n\u001b[1;32m    620\u001b[0m \u001b[38;5;124;03m          [See more information about frequency and presence penalties.](https://platform.openai.com/docs/guides/text-generation)\u001b[39;00m\n\u001b[1;32m    621\u001b[0m \n\u001b[1;32m    622\u001b[0m \u001b[38;5;124;03m      function_call: Deprecated in favor of `tool_choice`.\u001b[39;00m\n\u001b[1;32m    623\u001b[0m \n\u001b[1;32m    624\u001b[0m \u001b[38;5;124;03m          Controls which (if any) function is called by the model. `none` means the model\u001b[39;00m\n\u001b[1;32m    625\u001b[0m \u001b[38;5;124;03m          will not call a function and instead generates a message. `auto` means the model\u001b[39;00m\n\u001b[1;32m    626\u001b[0m \u001b[38;5;124;03m          can pick between generating a message or calling a function. Specifying a\u001b[39;00m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;124;03m          particular function via `{\"name\": \"my_function\"}` forces the model to call that\u001b[39;00m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;124;03m          function.\u001b[39;00m\n\u001b[1;32m    629\u001b[0m \n\u001b[1;32m    630\u001b[0m \u001b[38;5;124;03m          `none` is the default when no functions are present. `auto` is the default if\u001b[39;00m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;124;03m          functions are present.\u001b[39;00m\n\u001b[1;32m    632\u001b[0m \n\u001b[1;32m    633\u001b[0m \u001b[38;5;124;03m      functions: Deprecated in favor of `tools`.\u001b[39;00m\n\u001b[1;32m    634\u001b[0m \n\u001b[1;32m    635\u001b[0m \u001b[38;5;124;03m          A list of functions the model may generate JSON inputs for.\u001b[39;00m\n\u001b[1;32m    636\u001b[0m \n\u001b[1;32m    637\u001b[0m \u001b[38;5;124;03m      logit_bias: Modify the likelihood of specified tokens appearing in the completion.\u001b[39;00m\n\u001b[1;32m    638\u001b[0m \n\u001b[1;32m    639\u001b[0m \u001b[38;5;124;03m          Accepts a JSON object that maps tokens (specified by their token ID in the\u001b[39;00m\n\u001b[1;32m    640\u001b[0m \u001b[38;5;124;03m          tokenizer) to an associated bias value from -100 to 100. Mathematically, the\u001b[39;00m\n\u001b[1;32m    641\u001b[0m \u001b[38;5;124;03m          bias is added to the logits generated by the model prior to sampling. The exact\u001b[39;00m\n\u001b[1;32m    642\u001b[0m \u001b[38;5;124;03m          effect will vary per model, but values between -1 and 1 should decrease or\u001b[39;00m\n\u001b[1;32m    643\u001b[0m \u001b[38;5;124;03m          increase likelihood of selection; values like -100 or 100 should result in a ban\u001b[39;00m\n\u001b[1;32m    644\u001b[0m \u001b[38;5;124;03m          or exclusive selection of the relevant token.\u001b[39;00m\n\u001b[1;32m    645\u001b[0m \n\u001b[1;32m    646\u001b[0m \u001b[38;5;124;03m      logprobs: Whether to return log probabilities of the output tokens or not. If true,\u001b[39;00m\n\u001b[1;32m    647\u001b[0m \u001b[38;5;124;03m          returns the log probabilities of each output token returned in the `content` of\u001b[39;00m\n\u001b[1;32m    648\u001b[0m \u001b[38;5;124;03m          `message`.\u001b[39;00m\n\u001b[1;32m    649\u001b[0m \n\u001b[1;32m    650\u001b[0m \u001b[38;5;124;03m      max_completion_tokens: An upper bound for the number of tokens that can be generated for a completion,\u001b[39;00m\n\u001b[1;32m    651\u001b[0m \u001b[38;5;124;03m          including visible output tokens and\u001b[39;00m\n\u001b[1;32m    652\u001b[0m \u001b[38;5;124;03m          [reasoning tokens](https://platform.openai.com/docs/guides/reasoning).\u001b[39;00m\n\u001b[1;32m    653\u001b[0m \n\u001b[1;32m    654\u001b[0m \u001b[38;5;124;03m      max_tokens: The maximum number of [tokens](/tokenizer) that can be generated in the chat\u001b[39;00m\n\u001b[1;32m    655\u001b[0m \u001b[38;5;124;03m          completion. This value can be used to control\u001b[39;00m\n\u001b[1;32m    656\u001b[0m \u001b[38;5;124;03m          [costs](https://openai.com/api/pricing/) for text generated via API.\u001b[39;00m\n\u001b[1;32m    657\u001b[0m \n\u001b[1;32m    658\u001b[0m \u001b[38;5;124;03m          This value is now deprecated in favor of `max_completion_tokens`, and is not\u001b[39;00m\n\u001b[1;32m    659\u001b[0m \u001b[38;5;124;03m          compatible with\u001b[39;00m\n\u001b[1;32m    660\u001b[0m \u001b[38;5;124;03m          [o1 series models](https://platform.openai.com/docs/guides/reasoning).\u001b[39;00m\n\u001b[1;32m    661\u001b[0m \n\u001b[1;32m    662\u001b[0m \u001b[38;5;124;03m      metadata: Developer-defined tags and values used for filtering completions in the\u001b[39;00m\n\u001b[1;32m    663\u001b[0m \u001b[38;5;124;03m          [dashboard](https://platform.openai.com/chat-completions).\u001b[39;00m\n\u001b[1;32m    664\u001b[0m \n\u001b[1;32m    665\u001b[0m \u001b[38;5;124;03m      modalities: Output types that you would like the model to generate for this request. Most\u001b[39;00m\n\u001b[1;32m    666\u001b[0m \u001b[38;5;124;03m          models are capable of generating text, which is the default:\u001b[39;00m\n\u001b[1;32m    667\u001b[0m \n\u001b[1;32m    668\u001b[0m \u001b[38;5;124;03m          `[\"text\"]`\u001b[39;00m\n\u001b[1;32m    669\u001b[0m \n\u001b[1;32m    670\u001b[0m \u001b[38;5;124;03m          The `gpt-4o-audio-preview` model can also be used to\u001b[39;00m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;124;03m          [generate audio](https://platform.openai.com/docs/guides/audio). To request that\u001b[39;00m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;124;03m          this model generate both text and audio responses, you can use:\u001b[39;00m\n\u001b[1;32m    673\u001b[0m \n\u001b[1;32m    674\u001b[0m \u001b[38;5;124;03m          `[\"text\", \"audio\"]`\u001b[39;00m\n\u001b[1;32m    675\u001b[0m \n\u001b[1;32m    676\u001b[0m \u001b[38;5;124;03m      n: How many chat completion choices to generate for each input message. Note that\u001b[39;00m\n\u001b[1;32m    677\u001b[0m \u001b[38;5;124;03m          you will be charged based on the number of generated tokens across all of the\u001b[39;00m\n\u001b[1;32m    678\u001b[0m \u001b[38;5;124;03m          choices. Keep `n` as `1` to minimize costs.\u001b[39;00m\n\u001b[1;32m    679\u001b[0m \n\u001b[1;32m    680\u001b[0m \u001b[38;5;124;03m      parallel_tool_calls: Whether to enable\u001b[39;00m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;124;03m          [parallel function calling](https://platform.openai.com/docs/guides/function-calling#configuring-parallel-function-calling)\u001b[39;00m\n\u001b[1;32m    682\u001b[0m \u001b[38;5;124;03m          during tool use.\u001b[39;00m\n\u001b[1;32m    683\u001b[0m \n\u001b[1;32m    684\u001b[0m \u001b[38;5;124;03m      prediction: Static predicted output content, such as the content of a text file that is\u001b[39;00m\n\u001b[1;32m    685\u001b[0m \u001b[38;5;124;03m          being regenerated.\u001b[39;00m\n\u001b[1;32m    686\u001b[0m \n\u001b[1;32m    687\u001b[0m \u001b[38;5;124;03m      presence_penalty: Number between -2.0 and 2.0. Positive values penalize new tokens based on\u001b[39;00m\n\u001b[1;32m    688\u001b[0m \u001b[38;5;124;03m          whether they appear in the text so far, increasing the model's likelihood to\u001b[39;00m\n\u001b[1;32m    689\u001b[0m \u001b[38;5;124;03m          talk about new topics.\u001b[39;00m\n\u001b[1;32m    690\u001b[0m \n\u001b[1;32m    691\u001b[0m \u001b[38;5;124;03m          [See more information about frequency and presence penalties.](https://platform.openai.com/docs/guides/text-generation)\u001b[39;00m\n\u001b[1;32m    692\u001b[0m \n\u001b[1;32m    693\u001b[0m \u001b[38;5;124;03m      response_format: An object specifying the format that the model must output. Compatible with\u001b[39;00m\n\u001b[1;32m    694\u001b[0m \u001b[38;5;124;03m          [GPT-4o](https://platform.openai.com/docs/models#gpt-4o),\u001b[39;00m\n\u001b[1;32m    695\u001b[0m \u001b[38;5;124;03m          [GPT-4o mini](https://platform.openai.com/docs/models#gpt-4o-mini),\u001b[39;00m\n\u001b[1;32m    696\u001b[0m \u001b[38;5;124;03m          [GPT-4 Turbo](https://platform.openai.com/docs/models#gpt-4-turbo-and-gpt-4) and\u001b[39;00m\n\u001b[1;32m    697\u001b[0m \u001b[38;5;124;03m          all GPT-3.5 Turbo models newer than `gpt-3.5-turbo-1106`.\u001b[39;00m\n\u001b[1;32m    698\u001b[0m \n\u001b[1;32m    699\u001b[0m \u001b[38;5;124;03m          Setting to `{ \"type\": \"json_schema\", \"json_schema\": {...} }` enables Structured\u001b[39;00m\n\u001b[1;32m    700\u001b[0m \u001b[38;5;124;03m          Outputs which ensures the model will match your supplied JSON schema. Learn more\u001b[39;00m\n\u001b[1;32m    701\u001b[0m \u001b[38;5;124;03m          in the\u001b[39;00m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;124;03m          [Structured Outputs guide](https://platform.openai.com/docs/guides/structured-outputs).\u001b[39;00m\n\u001b[1;32m    703\u001b[0m \n\u001b[1;32m    704\u001b[0m \u001b[38;5;124;03m          Setting to `{ \"type\": \"json_object\" }` enables JSON mode, which ensures the\u001b[39;00m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;124;03m          message the model generates is valid JSON.\u001b[39;00m\n\u001b[1;32m    706\u001b[0m \n\u001b[1;32m    707\u001b[0m \u001b[38;5;124;03m          **Important:** when using JSON mode, you **must** also instruct the model to\u001b[39;00m\n\u001b[1;32m    708\u001b[0m \u001b[38;5;124;03m          produce JSON yourself via a system or user message. Without this, the model may\u001b[39;00m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;124;03m          generate an unending stream of whitespace until the generation reaches the token\u001b[39;00m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;124;03m          limit, resulting in a long-running and seemingly \"stuck\" request. Also note that\u001b[39;00m\n\u001b[1;32m    711\u001b[0m \u001b[38;5;124;03m          the message content may be partially cut off if `finish_reason=\"length\"`, which\u001b[39;00m\n\u001b[1;32m    712\u001b[0m \u001b[38;5;124;03m          indicates the generation exceeded `max_tokens` or the conversation exceeded the\u001b[39;00m\n\u001b[1;32m    713\u001b[0m \u001b[38;5;124;03m          max context length.\u001b[39;00m\n\u001b[1;32m    714\u001b[0m \n\u001b[1;32m    715\u001b[0m \u001b[38;5;124;03m      seed: This feature is in Beta. If specified, our system will make a best effort to\u001b[39;00m\n\u001b[1;32m    716\u001b[0m \u001b[38;5;124;03m          sample deterministically, such that repeated requests with the same `seed` and\u001b[39;00m\n\u001b[1;32m    717\u001b[0m \u001b[38;5;124;03m          parameters should return the same result. Determinism is not guaranteed, and you\u001b[39;00m\n\u001b[1;32m    718\u001b[0m \u001b[38;5;124;03m          should refer to the `system_fingerprint` response parameter to monitor changes\u001b[39;00m\n\u001b[1;32m    719\u001b[0m \u001b[38;5;124;03m          in the backend.\u001b[39;00m\n\u001b[1;32m    720\u001b[0m \n\u001b[1;32m    721\u001b[0m \u001b[38;5;124;03m      service_tier: Specifies the latency tier to use for processing the request. This parameter is\u001b[39;00m\n\u001b[1;32m    722\u001b[0m \u001b[38;5;124;03m          relevant for customers subscribed to the scale tier service:\u001b[39;00m\n\u001b[1;32m    723\u001b[0m \n\u001b[1;32m    724\u001b[0m \u001b[38;5;124;03m          - If set to 'auto', and the Project is Scale tier enabled, the system will\u001b[39;00m\n\u001b[1;32m    725\u001b[0m \u001b[38;5;124;03m            utilize scale tier credits until they are exhausted.\u001b[39;00m\n\u001b[1;32m    726\u001b[0m \u001b[38;5;124;03m          - If set to 'auto', and the Project is not Scale tier enabled, the request will\u001b[39;00m\n\u001b[1;32m    727\u001b[0m \u001b[38;5;124;03m            be processed using the default service tier with a lower uptime SLA and no\u001b[39;00m\n\u001b[1;32m    728\u001b[0m \u001b[38;5;124;03m            latency guarentee.\u001b[39;00m\n\u001b[1;32m    729\u001b[0m \u001b[38;5;124;03m          - If set to 'default', the request will be processed using the default service\u001b[39;00m\n\u001b[1;32m    730\u001b[0m \u001b[38;5;124;03m            tier with a lower uptime SLA and no latency guarentee.\u001b[39;00m\n\u001b[1;32m    731\u001b[0m \u001b[38;5;124;03m          - When not set, the default behavior is 'auto'.\u001b[39;00m\n\u001b[1;32m    732\u001b[0m \n\u001b[1;32m    733\u001b[0m \u001b[38;5;124;03m          When this parameter is set, the response body will include the `service_tier`\u001b[39;00m\n\u001b[1;32m    734\u001b[0m \u001b[38;5;124;03m          utilized.\u001b[39;00m\n\u001b[1;32m    735\u001b[0m \n\u001b[1;32m    736\u001b[0m \u001b[38;5;124;03m      stop: Up to 4 sequences where the API will stop generating further tokens.\u001b[39;00m\n\u001b[1;32m    737\u001b[0m \n\u001b[1;32m    738\u001b[0m \u001b[38;5;124;03m      store: Whether or not to store the output of this chat completion request for use in\u001b[39;00m\n\u001b[1;32m    739\u001b[0m \u001b[38;5;124;03m          our [model distillation](https://platform.openai.com/docs/guides/distillation)\u001b[39;00m\n\u001b[1;32m    740\u001b[0m \u001b[38;5;124;03m          or [evals](https://platform.openai.com/docs/guides/evals) products.\u001b[39;00m\n\u001b[1;32m    741\u001b[0m \n\u001b[0;32m--> 742\u001b[0m \u001b[38;5;124;03m      stream_options: Options for streaming response. Only set this when you set `stream: true`.\u001b[39;00m\n\u001b[1;32m    743\u001b[0m \n\u001b[1;32m    744\u001b[0m \u001b[38;5;124;03m      temperature: What sampling temperature to use, between 0 and 2. Higher values like 0.8 will\u001b[39;00m\n\u001b[1;32m    745\u001b[0m \u001b[38;5;124;03m          make the output more random, while lower values like 0.2 will make it more\u001b[39;00m\n\u001b[1;32m    746\u001b[0m \u001b[38;5;124;03m          focused and deterministic.\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \n\u001b[1;32m    748\u001b[0m \u001b[38;5;124;03m          We generally recommend altering this or `top_p` but not both.\u001b[39;00m\n\u001b[1;32m    749\u001b[0m \n\u001b[1;32m    750\u001b[0m \u001b[38;5;124;03m      tool_choice: Controls which (if any) tool is called by the model. `none` means the model will\u001b[39;00m\n\u001b[1;32m    751\u001b[0m \u001b[38;5;124;03m          not call any tool and instead generates a message. `auto` means the model can\u001b[39;00m\n\u001b[1;32m    752\u001b[0m \u001b[38;5;124;03m          pick between generating a message or calling one or more tools. `required` means\u001b[39;00m\n\u001b[1;32m    753\u001b[0m \u001b[38;5;124;03m          the model must call one or more tools. Specifying a particular tool via\u001b[39;00m\n\u001b[1;32m    754\u001b[0m \u001b[38;5;124;03m          `{\"type\": \"function\", \"function\": {\"name\": \"my_function\"}}` forces the model to\u001b[39;00m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;124;03m          call that tool.\u001b[39;00m\n\u001b[1;32m    756\u001b[0m \n\u001b[1;32m    757\u001b[0m \u001b[38;5;124;03m          `none` is the default when no tools are present. `auto` is the default if tools\u001b[39;00m\n\u001b[1;32m    758\u001b[0m \u001b[38;5;124;03m          are present.\u001b[39;00m\n\u001b[1;32m    759\u001b[0m \n\u001b[1;32m    760\u001b[0m \u001b[38;5;124;03m      tools: A list of tools the model may call. Currently, only functions are supported as a\u001b[39;00m\n\u001b[1;32m    761\u001b[0m \u001b[38;5;124;03m          tool. Use this to provide a list of functions the model may generate JSON inputs\u001b[39;00m\n\u001b[1;32m    762\u001b[0m \u001b[38;5;124;03m          for. A max of 128 functions are supported.\u001b[39;00m\n\u001b[1;32m    763\u001b[0m \n\u001b[1;32m    764\u001b[0m \u001b[38;5;124;03m      top_logprobs: An integer between 0 and 20 specifying the number of most likely tokens to\u001b[39;00m\n\u001b[1;32m    765\u001b[0m \u001b[38;5;124;03m          return at each token position, each with an associated log probability.\u001b[39;00m\n\u001b[1;32m    766\u001b[0m \u001b[38;5;124;03m          `logprobs` must be set to `true` if this parameter is used.\u001b[39;00m\n\u001b[1;32m    767\u001b[0m \n\u001b[1;32m    768\u001b[0m \u001b[38;5;124;03m      top_p: An alternative to sampling with temperature, called nucleus sampling, where the\u001b[39;00m\n\u001b[1;32m    769\u001b[0m \u001b[38;5;124;03m          model considers the results of the tokens with top_p probability mass. So 0.1\u001b[39;00m\n\u001b[1;32m    770\u001b[0m \u001b[38;5;124;03m          means only the tokens comprising the top 10% probability mass are considered.\u001b[39;00m\n\u001b[1;32m    771\u001b[0m \n\u001b[1;32m    772\u001b[0m \u001b[38;5;124;03m          We generally recommend altering this or `temperature` but not both.\u001b[39;00m\n\u001b[1;32m    773\u001b[0m \n\u001b[1;32m    774\u001b[0m \u001b[38;5;124;03m      user: A unique identifier representing your end-user, which can help OpenAI to monitor\u001b[39;00m\n\u001b[1;32m    775\u001b[0m \u001b[38;5;124;03m          and detect abuse.\u001b[39;00m\n\u001b[1;32m    776\u001b[0m \u001b[38;5;124;03m          [Learn more](https://platform.openai.com/docs/guides/safety-best-practices#end-user-ids).\u001b[39;00m\n\u001b[1;32m    777\u001b[0m \n\u001b[1;32m    778\u001b[0m \u001b[38;5;124;03m      extra_headers: Send extra headers\u001b[39;00m\n\u001b[1;32m    779\u001b[0m \n\u001b[1;32m    780\u001b[0m \u001b[38;5;124;03m      extra_query: Add additional query parameters to the request\u001b[39;00m\n\u001b[1;32m    781\u001b[0m \n\u001b[1;32m    782\u001b[0m \u001b[38;5;124;03m      extra_body: Add additional JSON properties to the request\u001b[39;00m\n\u001b[1;32m    783\u001b[0m \n\u001b[1;32m    784\u001b[0m \u001b[38;5;124;03m      timeout: Override the client-level default timeout for this request, in seconds\u001b[39;00m\n\u001b[1;32m    785\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    786\u001b[0m     \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/llm-gateway/lib/python3.12/site-packages/openai/_base_client.py:1277\u001b[0m, in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1266\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1267\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1268\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1275\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1276\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m-> 1277\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1278\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1279\u001b[0m     )\n\u001b[1;32m   1280\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls))\n",
      "File \u001b[0;32m~/miniconda3/envs/llm-gateway/lib/python3.12/site-packages/openai/_base_client.py:954\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    952\u001b[0m if remaining_retries is not None:\n\u001b[1;32m    953\u001b[0m     retries_taken = options.get_max_retries(self.max_retries) - remaining_retries\n\u001b[0;32m--> 954\u001b[0m else:\n\u001b[1;32m    955\u001b[0m     retries_taken = 0\n\u001b[1;32m    957\u001b[0m return self._request(\n\u001b[1;32m    958\u001b[0m     cast_to=cast_to,\n\u001b[1;32m    959\u001b[0m     options=options,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    962\u001b[0m     retries_taken=retries_taken,\n\u001b[1;32m    963\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/llm-gateway/lib/python3.12/site-packages/openai/_base_client.py:1058\u001b[0m, in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1055\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m   1056\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m   1057\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n\u001b[0;32m-> 1058\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1060\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': 'Budget has been exceeded! Current cost: 5.845, Max budget: 5.0', 'type': 'budget_exceeded', 'param': None, 'code': '400'}}"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"Meta_Llama_31_8b_it\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Parle moi de la seconde guerre mondiale avec un max de dÃ©tails\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from litellm import token_counter\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": \"Hey, how's it going\"}]\n",
    "token_count = token_counter(model=\"gpt-3.5-turbo\", messages=messages)\n",
    "print(f\"Token count: {token_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lucie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La RÃ©volution franÃ§aise de 1789 fut un Ã©vÃ©nement majeur de l'histoire de France et du monde. Elle a commencÃ© avec la prise de la Bastille le 14 juillet 1789 et s'est achevÃ©e avec la proclamation de la DÃ©claration des droits de l'Homme et du Citoyen le 26 aoÃ»t 1789.\n",
      "\n",
      "La RÃ©volution franÃ§aise a eu de nombreuses causes, telles que la montÃ©e de l'absolutisme royal, la crise financiÃ¨re et Ã©conomique, l'inÃ©galitÃ© sociale et les tensions politiques. Elle a Ã©galement Ã©tÃ© influencÃ©e par les idÃ©es des LumiÃ¨res et de la philosophie radicale.\n",
      "\n",
      "La RÃ©volution franÃ§aise a entraÃ®nÃ© de nombreux changements, notamment la fin de la monarchie absolue et l'Ã©tablissement de la RÃ©publique. Elle a Ã©galement conduit Ã  la DÃ©claration des droits de l'Homme et du Citoyen, qui a Ã©tabli des principes fondamentaux tels que la libertÃ©, l'Ã©galitÃ© et la fraternitÃ©.\n",
      "\n",
      "Cependant, la RÃ©volution franÃ§aise a Ã©galement Ã©tÃ© marquÃ©e par la violence et la terreur, avec l'exÃ©cution de nombreux nobles et la mise en place de la Terreur par Robespierre.\n",
      "\n",
      "La RÃ©volution franÃ§aise a eu un impact majeur sur l'histoire de France et du monde, et continue d'Ãªtre Ã©tudiÃ©e et discutÃ©e Ã  ce jour.\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"Lucie_7B\",\n",
    "    temperature=0.6,\n",
    "    user='user3@maif.com',\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Parle moi de la rÃ©volution franÃ§aise\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chat-58e6116d091c4f06b7f59a6148f97225', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"La RÃ©volution franÃ§aise de 1789 fut un Ã©vÃ©nement majeur de l'histoire de France et du monde. Elle a commencÃ© avec la prise de la Bastille le 14 juillet 1789 et s'est achevÃ©e avec la proclamation de la DÃ©claration des droits de l'Homme et du Citoyen le 26 aoÃ»t 1789.\\n\\nLa RÃ©volution franÃ§aise a eu de nombreuses causes, telles que la montÃ©e de l'absolutisme royal, la crise financiÃ¨re et Ã©conomique, l'inÃ©galitÃ© sociale et les tensions politiques. Elle a Ã©galement Ã©tÃ© influencÃ©e par les idÃ©es des LumiÃ¨res et de la philosophie radicale.\\n\\nLa RÃ©volution franÃ§aise a entraÃ®nÃ© de nombreux changements, notamment la fin de la monarchie absolue et l'Ã©tablissement de la RÃ©publique. Elle a Ã©galement conduit Ã  la DÃ©claration des droits de l'Homme et du Citoyen, qui a Ã©tabli des principes fondamentaux tels que la libertÃ©, l'Ã©galitÃ© et la fraternitÃ©.\\n\\nCependant, la RÃ©volution franÃ§aise a Ã©galement Ã©tÃ© marquÃ©e par la violence et la terreur, avec l'exÃ©cution de nombreux nobles et la mise en place de la Terreur par Robespierre.\\n\\nLa RÃ©volution franÃ§aise a eu un impact majeur sur l'histoire de France et du monde, et continue d'Ãªtre Ã©tudiÃ©e et discutÃ©e Ã  ce jour.\", refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1733754016, model='lucie-7b', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=258, prompt_tokens=17, total_tokens=275, completion_tokens_details=None, prompt_tokens_details=None))\n"
     ]
    }
   ],
   "source": [
    "response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curl -X GET 'http://localhost:4000/global/spend/report?start_date=2024-12-01&end_date=2024-12-31&group_by=team' \\\n",
    "  -H 'Authorization: Bearer sk-1234'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "url = 'http://localhost:4000/global/spend/report'\n",
    "params = {\n",
    "    'start_date': '2024-12-01',\n",
    "    'end_date': '2024-12-30'\n",
    "}\n",
    "\n",
    "headers = {\n",
    "    'Authorization': 'Bearer sk-1234'\n",
    "}\n",
    "\n",
    "# Make the GET request\n",
    "response = requests.get(url, headers=headers, params=params)\n",
    "spend_report = response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Get list of models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"data\":[{\"id\":\"Meta_Llama_31_8b_it\",\"object\":\"model\",\"created\":1677610602,\"owned_by\":\"openai\"},{\"id\":\"Lucie_7B\",\"object\":\"model\",\"created\":1677610602,\"owned_by\":\"openai\"}],\"object\":\"list\"}"
     ]
    }
   ],
   "source": [
    "!curl -X 'GET' \\\n",
    "  'http://localhost:4000/v1/models' \\\n",
    "  -H 'accept: application/json' \\\n",
    "  -H 'Authorization: Bearer sk-1234'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Generate key via api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<string>, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m<string>:10\u001b[0;36m\u001b[0m\n\u001b[0;31m    }'\u001b[0m\n\u001b[0m      ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "!curl -X 'POST' \\\n",
    "  'http://localhost:4000/key/generate' \\\n",
    "  -H 'accept: application/json' \\\n",
    "  -H 'Content-Type: application/json' \\\n",
    "  -H 'Authorization: Bearer sk-1234' \\\n",
    "  -d '{\n",
    "    \"key_alias\": \"Sales\",\n",
    "    \"models\": [\"Lucie_7B\"],\n",
    "    \"max_budget\": 10,\n",
    "    \"user_id\": \"user1_id\",\n",
    "    \"team_id\": \"team1_id\",\n",
    "    \"max_parallel_requests\": 10,\n",
    "    \"blocked\": true,\n",
    "    \"aliases\": {}\n",
    "  }'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"keys\":[\"88dc28d0f030c55ed4ab77ed8faf098196cb1c05df778539800c9f1243fe6b4b\",\"983002003de2c682d18c86f0f450f9b3469d88021e7a8e01bb28ba46adeff48a\",\"d1117f612d7a35a93e69584ff2d595aa603e598e980dd4d1fd9640b5f884b1f2\"],\"total_count\":3,\"current_page\":1,\"total_pages\":1}curl: (6) Could not resolve host: \\\n"
     ]
    }
   ],
   "source": [
    "!curl -X 'GET' \\\n",
    "  'http://localhost:4000/key/list?page=1&size=10' \\\n",
    "  -H 'accept: application/json' \\\n",
    "  -H 'Authorization: Bearer sk-1234' \\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]"
     ]
    }
   ],
   "source": [
    "!curl -X GET \"http://0.0.0.0:4000/spend/tags\" \\\n",
    "-H \"Authorization: Bearer sk-1234\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"error\":{\"message\":\"Authentication Error, No api key passed in.\",\"type\":\"auth_error\",\"param\":\"None\",\"code\":\"401\"}}"
     ]
    }
   ],
   "source": [
    "!curl -X 'GET' \\\n",
    "  'http://localhost:4000/global/spend/report?start_date=2024-12-02&end_date=2024-12-10&group_by=MAIF&api_key=sk-1234&internal_user_id=user1' \\\n",
    "  -H 'accept: application/json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pegasus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
